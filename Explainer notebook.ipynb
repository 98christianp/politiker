{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Danish Politics: Tweeting vs. meeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Christian Engelbrekt Petersen, Frederik Kromann Hansen & Lau Johansson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Christian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Christian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Christian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Christian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tweepy\n",
    "import re #https://developers.google.com/edu/python/regular-expressions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches #legends\n",
    "from matplotlib.lines import Line2D #circles as legends\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('pos_tag')\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import community\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from fa2 import ForceAtlas2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe datasets (goal -> what is dataset -> how does it answer goal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Twitter  (LAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On [twitterpolitikere.dk](https://filip.journet.sdu.dk/twitter/politikere/), twitterprofiles of 650 danish politicians has been collected (25 november 2020). The publishers of the data is journalists Ernst Poulsen and Filip Walberg in collaboration with Center for Journalism. The website is continously updated by the publishers, where data from Twitters open APIs. Danish politicians can choose to write to the publishers to be removed from the list. Each person's profile is assessed manually before we include people on the lists. There are no information on process of assesing the profiles. If interested, this [link](https://twitter.com/ernstpoulsen/lists) has more lists with overviews of people in different industries: Politicians, journalists and people in managerial positions, etc.\n",
    "\n",
    "From the list of danish politicians' twitternames, each of the profiles is going to represent a node in a graph that is going to be called: \"Danish Politicians Twitter Network\". In order to make a network which connects the danish politicians, this project uses tweets from Twitter. Initially a developer account for twitter has been created. Tweet can then be assesed through Twitters API on https://developer.twitter.com/en. [Tweets](https://help.twitter.com/en/using-twitter/types-of-tweets) are messages posted to Twitter containing text, photos, a GIF, and/or video. Both sender and receivers of tweets can see all tweets on a Home Timeline - a timeline that is different for each person - depending on who you are following. \n",
    "\n",
    "This project is especially interested in [mentions](https://help.twitter.com/en/using-twitter/types-of-tweets), because mentions are tweets that contains other profiles twitter usernames. A mention in a tweet is preceded by the \"@\" symbol - which makes it possible to examine the mentions between danish politicians. These mentions are the connections between politicians in the \"Danish Politicians Twitter Network\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Politicians (parties and regions) (CHRISTIAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, politicians are segregated into different parties that align the best with their idealogies, since this is of importance to the project, we had to find a way to get these as well. In Denmark, we also have 5 main regions. If you know american politics, then you can kind off think of these as your states. As we want to investigate how politiicans interact, these labels are essential, as we wish to answer how parties interact and allign on SoMe and in folketingets meetings.\n",
    "\n",
    "![Regioner i dk](https://rn.dk/-/media/Rn_dk/Genveje/Fakta-om-Nordjylland/Regioner-i-Danmark/Danmark_Regioner_2017.ashx?la=da)\n",
    "The above image shows the regions and population in each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Political meetings (FREDERIK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitterprofiles  (LAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially all of the twitterprofiles are collected using the webpage https://filip.journet.sdu.dk/twitter/politikere/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "URL = 'https://filip.journet.sdu.dk/twitter/politikere/'\n",
    "page = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the webpage, is it concluded that all twitternames are displayed in < h3 > classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_names = soup.find_all('h3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list to save all the twitter names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_name_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping all of the names from the webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for names in twitter_names:\n",
    "    match=re.search(\"<h3>#\",str(names))\n",
    "    if match:\n",
    "        result_name=re.findall(\".com/\\w*\",str(names))[0].replace(\".com/\",\"\")\n",
    "        twitter_name_list.append(result_name.rstrip())\n",
    "        #print(result_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the twitternames (twitterprofiles) a corresponding real name is shown. The real politician names are displayed in < small > classes. Initially a empty list for all the names are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_name_list=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereafter, the webpage are scraped in order to find the realnames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margrethe Vestager \n",
      "Lars Løkke Rasmussen \n",
      "Morten Østergaard \n",
      "Pernille Skipper \n",
      "Ida Auken \n",
      "Kristian Jensen \n",
      "Uffe Elbaek \n",
      "Søren Pape Poulsen \n",
      "Pia Olsen Dyhr \n",
      "Magnus Heunicke \n",
      "Sofie CarstenNielsen \n",
      "Statsministeriet \n",
      "Regeringen \n",
      "Brian Holm \n",
      "Simon Emil Ammitzbøll-Bille \n",
      "Jakob Ellemann-Jensen \n",
      "Dan Jørgensen \n",
      "Denmark MFA 🇩🇰 \n",
      "Mattias Tesfaye \n",
      "Rosenkrantz-Theil \n",
      "Ellen Trane Nørby \n",
      "Morten Messerschmidt \n",
      "Frank Jensen \n",
      "Sophie Løhde \n",
      "Tommy Ahlers \n",
      "Martin Lidegaard \n",
      "Karen Melchior😷🧼🇪🇺 \n",
      "Zenia Stampe \n",
      "Astrid Krag \n",
      "Benny Engelbrecht \n",
      "Rasmus Jarlov \n",
      "Peter Hummelgaard \n",
      "Jan E. Jørgensen \n",
      "Karsten Lauritzen \n",
      "Mogens Jensen \n",
      "Mette Abildgaard \n",
      "Morten Bødskov \n",
      "Kristian Thulesen Dahl \n",
      "Rasmus Prehn \n",
      "Alex Vanopslagh \n",
      "Per Clausen \n",
      "Jacob Mark \n",
      "Mai Mercado \n",
      "Jesper Petersen \n",
      "Peter Skaarup \n",
      "Nikolaj Villumsen \n",
      "Pernille Vermund \n",
      "Trine Bramsen \n",
      "Henrik Sass \n",
      "Margrete Auken \n",
      "Jens Rohde \n",
      "Mette Gjerskov \n",
      "Lotte Rod \n",
      "Morten Helveg \n",
      "Uddannelses- og Forskningsministeriet \n",
      "Søren Espersen \n",
      "Ole Birk Olesen \n",
      "Pia Kjærsgaard \n",
      "Rosa Lund \n",
      "Lisbeth Bech-Nielsen \n",
      "rasmus nordqvist 🏳️‍🌈 \n",
      "Sophie H. Andersen \n",
      "Jeppe Kofod \n",
      "Ane Halsboe \n",
      "Christel Schaldemose \n",
      "Finansministeriet \n",
      "Morten Løkkegaard \n",
      "Jakob Engel-Schmidt \n",
      "Simon Kollerup \n",
      "Kristian Hegaard \n",
      "Karsten Hønge \n",
      "Jens Joel \n",
      "Erhvervsministeriet \n",
      "Sundheds- og Ældreministeriet \n",
      "Torsten Schack \n",
      "Ulla Tørnæs \n",
      "Rasmus Horn Langhoff \n",
      "Michael Aastrup Jensen \n",
      "Peter Kofod(DF) \n",
      "Miljøministeriet \n",
      "Naser Khader \n",
      "Henrik Dahl \n",
      "Liselott Blixt \n",
      "Troels Lund Poulsen \n",
      "Nicolai Wammen \n",
      "Lars Chr. Lilleholt \n",
      "Andreas Steenberg \n",
      "Stinus Lindgreen \n",
      "Lea Wermelin \n",
      "Niko Grünfeld \n",
      "Kaare Dybvad \n",
      "Benedikte Kiær \n",
      "Jacob Bundsgaard \n",
      "Kåre Traberg Smidt \n",
      "Karen Ellemann \n",
      "Laura Lindahl \n",
      "EU-Parlamentet i DK \n",
      "Marianne Jelved \n",
      "Forsvarsministeriet/Danish MoD \n",
      "Kulturministeriet \n",
      "Bertel Haarder \n",
      "Mads Fuglede \n",
      "Klimaministeriet \n",
      "Samira Nawa \n",
      "Rasmus Helveg \n",
      "Joymogensen \n",
      "Børne- og Undervisningsministeriet \n",
      "Marie Krarup \n",
      "Kira M. Peter-Hansen \n",
      "Hans Andersen \n",
      "Jane Heitmann \n",
      "Lone Loklindt \n",
      "Christina Krzyrosiak Hansen \n",
      "Jacob Jensen \n",
      "Villy Søvndal \n",
      "Fatma Øktem \n",
      "Torsten Gejl \n",
      "Morten Dahlin \n",
      "Pernille Weiss \n",
      "Michael Ziegler \n",
      "Signe Munk \n",
      "Kenneth K. Berth \n",
      "Carl Valentin \n",
      "Stephanie Lose \n",
      "Kirsten Normann \n",
      "Trine Schøning Torp \n",
      "Jeppe Bruus \n",
      "Eva Kjer Hansen \n",
      "Karina Lorentzen D. \n",
      "Thomas Danielsen \n",
      "Camilla Schwalbe \n",
      "Louise Schack Elholm \n",
      "Kr. Pihl Lorentzen \n",
      "Simon Pihl Sørensen \n",
      "Sikandar Siddique \n",
      "Mai Villadsen \n",
      "Dennis Flydtkjær \n",
      "Mette Thiesen \n",
      "Britt Bager \n",
      "Christian Juhl \n",
      "Kasper Sand Kjær \n",
      "Skatteministeriet \n",
      "Anni Matthiesen \n",
      "Rune Lund \n",
      "Eva Flyvholm \n",
      "Casper Hedegaard \n",
      "Mona Juul \n",
      "Chr Rabjerg Madsen \n",
      "Karina Adsbøl \n",
      "Hans Kristian Skibby \n",
      "Rasmus Stoklund \n",
      "Christopher Røhl A. \n",
      "Transport- og Boligministeriet \n",
      "Linea Søgaard-Lidell \n",
      "Lars Krarup \n",
      "John Dyrby Paulsen \n",
      "Sisse Marie Welling \n",
      "Jakob Sølvhøj \n",
      "Søren Gade \n",
      "Leif Lahn Jensen \n",
      "Flemming M Mortensen \n",
      "Victoria Velásquez \n",
      "Niels Fuglsang \n",
      "Lars Boje Mathiesen \n",
      "Ruben Kidde \n",
      "Jacob Bjerregaard \n",
      "Thomas Medom \n",
      "Søren Søndergaard \n",
      "Tommy Petersen \n",
      "Morten Marinus \n",
      "Marianne Vind \n",
      "Julie Skovsby \n",
      "Anne Marie Geisler \n",
      "Daniel Panduro \n",
      "Gunvor Wibroe \n",
      "Jesper Christensen \n",
      "Daniel Nyboe Andersen \n",
      "MetteAnnelie \n",
      "Marlene Harpsøe 🇩🇰 \n",
      "Karin Friis Bach \n",
      "Camilla Fabricius \n",
      "Lise Bech 🇩🇰 DF \n",
      "Katrine Robsøe \n",
      "Peder Hvelplund \n",
      "Ninna Hedeager Olsen \n",
      "Nikolaj Bøgh \n",
      "Social- og Indenrigsministeriet \n",
      "Lars Gaardhøj \n",
      "Steen Christiansen \n",
      "Anne Ehrenreich \n",
      "MiaNyegaard \n",
      "Marie Stærke \n",
      "Annette Lind \n",
      "Bjørn Brandenborg \n",
      "Serdal Benli \n",
      "Thomas Gyldal \n",
      "Anne Paulin \n",
      "Ulrik Kohl \n",
      "Peter Westermann \n",
      "Brigitte K. Jerkel \n",
      "Jens-Kristian Lütken \n",
      "Lars Aslan \n",
      "Jørn Pedersen \n",
      "Michael Vindfeldt \n",
      "Winni Grosbøll \n",
      "Astrid Carøe \n",
      "Caroline Stage \n",
      "Heino Knudsen (S) \n",
      "Anne Valentina Berthelsen \n",
      "Anders Kühnau \n",
      "KatarinaAmmitzboell \n",
      "Anne Sophie Callesen \n",
      "Susanne Zimmer \n",
      "Martin Geertsen \n",
      "ThomasKastrup-Larsen \n",
      "Rikke Lauritzen \n",
      "J H Thulesen Dahl \n",
      "Claus Jørgensen \n",
      "Ulla Astman \n",
      "Pernille Bendixen \n",
      "Orla Hav \n",
      "Daniel Rugholm \n",
      "Erling Bonnesen \n",
      "Marie Bjerre \n",
      "Birgitte Bergman \n",
      "Stén Knuth, Idrætsordfører Venstre \n",
      "Jonas Bjørn Jensen \n",
      "Tidligere Økonomi- og Indenrigsministeriet \n",
      "Peter Juel Jensen \n",
      "Anders Kronborg \n",
      "Troels Ravn \n",
      "Nick Hækkerup \n",
      "Martin Damm \n",
      "Torben Hansen \n",
      "Asger Christensen \n",
      "Anja C Jensen \n",
      "Kathrine Olldag \n",
      "Susanne Eilersen \n",
      "Erik Høgh-Sørensen \n",
      "Franciska Rosenkilde \n",
      "Mette H. Dencker \n",
      "Aaja Chemnitz Larsen \n",
      "Villum Christensen \n",
      "Sonja Marie Jensen \n",
      "Bo Nygaard Larsen \n",
      "Maria Gudme \n",
      "Steen Holm Iversen \n",
      "Per Larsen \n",
      "Bünyamin Simsek \n",
      "Peter Seier Christensen \n",
      "Birgit S. Hansen \n",
      "Jacob Isøe Klærke \n",
      "Charlotte Riis Engelbrecht \n",
      "Emil Blücher \n",
      "Balder Mørk Andersen \n",
      "Birgitte Vind \n",
      "Per Olesen \n",
      "Tonni Hansen \n",
      "Astrid Aller \n",
      "Pernille Beckmann \n",
      "Christoffer Lilleholt \n",
      "Jakob Gorm \n",
      "Anders Johansson \n",
      "Christoffer Melson \n",
      "Fanny Broholm \n",
      "Merete Scheelsbeck \n",
      "Niels Peter Bøgballe \n",
      "Daniel Prehn \n",
      "Hanne Roed \n",
      "Allan Ahmad \n",
      "SusanneUrsulaCrawley \n",
      "Halime Oguz \n",
      "Lasse Frimand Jensen \n",
      "Marc Perera Christensen \n",
      "Lasse Hansen \n",
      "Kirsten Rask \n",
      "Jens Mandrup \n",
      "Flemming Pless \n",
      "Flemming Damgaard \n",
      "Orla Østerby \n",
      "Hans Christian Schmidt \n",
      "Heidi Bank \n",
      "Marlene Ambo-Rasmussen \n",
      "Camilla Pedersen \n",
      "Lisbeth Lauritsen \n",
      "Daniel Toft Jakobsen \n",
      "Niels E. Bjerrum \n",
      "Henrik Hvidesten \n",
      "Lene Horsbøl \n",
      "Steen Wrist Ørts \n",
      "Carl Andersen \n",
      "Charlotte Lund \n",
      "Mogens Gade \n",
      "Jonathan SH. Nielsen \n",
      "Kristian Guldfeldt \n",
      "Jesper F. Rasmussen \n",
      "Jakob Lose \n",
      "LoneYalcinkaya \n",
      "Brian Dybro \n",
      "Morten Jung \n",
      "Leon Sebbelin \n",
      "Sine Heltberg \n",
      "Karoline Vind \n",
      "Simon Simonsen \n",
      "Niels Flemming Hansen \n",
      "Malte Larsen \n",
      "Marcus Vesterager \n",
      "Kim Valentin \n",
      "Merete Due Paarup \n",
      "Carsten Kissmeyer \n",
      "Jesper Kiel \n",
      "Maria Sloth \n",
      "Sofie Seidenfaden \n",
      "Lasse P. N. Olsen \n",
      "Anne Honoré Østergaard \n",
      "Rasmus Foged \n",
      "Annika Smith \n",
      "Kristian Nørgaard \n",
      "Annette Roed \n",
      "Andreas Boesen \n",
      "Özkan Kocak \n",
      "Niels Hörup (V) \n",
      "Kasper Fuhr \n",
      "Christian Donatzky \n",
      "Rose Sloth Hansen \n",
      "Anders Broholm \n",
      "Latifa Ljørring \n",
      "Rune Højer \n",
      "Bjarne Laustsen \n",
      "Kurt Damsted \n",
      "Johan Brødsgaard \n",
      "Tim Vermund \n",
      "Glen Madsen \n",
      "Michael Vajhøj \n",
      "Helle Adelborg \n",
      "Helle Bonnesen (C) \n",
      "Jeppe Trolle \n",
      "Chr. Wedell-Neegaard \n",
      "Rasmus Norup \n",
      "Qasam Nazir Ahmad \n",
      "Steen Hasselriis \n",
      "Fasael Rehman \n",
      "Klaus Mygind \n",
      "Stephan Kleinschmidt \n",
      "Heidi Wang \n",
      "Christian Brøns \n",
      "Thomas Krarup \n",
      "John Engelhardt \n",
      "Claus Houden \n",
      "Elisabeth Ildal \n",
      "Torben Lollike \n",
      "Gert Bjerregaard \n",
      "Kisser Franciska \n",
      "Mark Grossmann \n",
      "Peter Sørensen \n",
      "Hans Toft \n",
      "Jan Johansen \n",
      "Camilla Hove Lund \n",
      "Karina Vestergård \n",
      "Jeanne Toxværd \n",
      "Søren Steen Andersen \n",
      "Mogens Vad \n",
      "Steen B. Andersen \n",
      "Klaus Markussen \n",
      "Julie Gottschalk \n",
      "Timo Jensen \n",
      "Bodil Kornbek \n",
      "Søren Rasmussen \n",
      "Poul V. Jensen \n",
      "Jens Ive \n",
      "Kasandra Behrndt \n",
      "Susan Kjeldgaard \n",
      "ango winther \n",
      "Martin Bech \n",
      "Michael Krogh \n",
      "Jeppe Lindberg \n",
      "Laura Rosenvinge \n",
      "Marco Oehlenschläger \n",
      "KARIN SØJBERG HOLST \n",
      "Christine Dal \n",
      "Torben Elsig \n",
      "Per Zeidler \n",
      "Mette Engelbrecht \n",
      "Hjalte Daniel Hansen \n",
      "Anders Winnerskjold \n",
      "Hans Jørgen Hansen \n",
      "Jan Ravn Christensen \n",
      "Trine Madsen \n",
      "Simon Strange \n",
      "Trine Græse \n",
      "Mads Andersen \n",
      "Sjurdur Skaale \n",
      "Søren Rishøj Jakobsen \n",
      "Dennis Schmock \n",
      "Maria Baagøe Bove \n",
      "Louise Feilberg \n",
      "Rikke Mortensen \n",
      "Anne Jeremiassen \n",
      "Tanja Larsson \n",
      "Jesper Bitsch Bierbaum \n",
      "Mette Bossen Linnet \n",
      "Bo Vesth \n",
      "Miki Dam Larsen \n",
      "Jesper Clausson (S) \n",
      "Mads Duedahl \n",
      "Susan Kronborg \n",
      "Kristoffer H Storm \n",
      "Daniel Borup \n",
      "Githa Nelander \n",
      "Thomas Andresen \n",
      "Søren Hillers \n",
      "Ole Vive \n",
      "søren brink \n",
      "line ervolder \n",
      "Ole Bondo Christense \n",
      "Bodil Sø \n",
      "Maja Torp \n",
      "Tina French Nielsen \n",
      "Jan Wammen \n",
      "Benny Dall \n",
      "Claus Kjeldsen \n",
      "Tidligere konto MBLIS \n",
      "Torben Overgaard \n",
      "Kasper Glyngø \n",
      "Mikael Smed \n",
      "Steen Vindum \n",
      "Maja Krog \n",
      "Søren Kusk \n",
      "Tage Nielsen \n",
      "Kåre Harder Olesen \n",
      "Gitte Simoni \n",
      "Henrik Moeller \n",
      "Morten Slotved \n",
      "Claus Omann Jensen \n",
      "Gorm Gunnarsen \n",
      "H.C. Østerby \n",
      "Rasmus Brask \n",
      "Diana Mose Olsen \n",
      "Ida Damborg \n",
      "Marianne Mathiesen \n",
      "Søren Schow \n",
      "Steffen Mølgaard \n",
      "Kristian Dyhr \n",
      "Peter Mikkelsen \n",
      "Robert Sørensen \n",
      "Morten Andersen \n",
      "Hans Holmer \n",
      "Anna Hede \n",
      "William Sonne Kaalø \n",
      "Preben Friis-Hauge \n",
      "Anne-Mette Ebensgaard🌹ame@odense.dk Tel: 21327352 \n",
      "Tommy Hummelmose \n",
      "Lone Hindø \n",
      "Lars Hoppe Søe \n",
      "Karen Sandrini \n",
      "Sascha Qvortrup \n",
      "Otto Kjær Larsen \n",
      "Søren Greve \n",
      "peter duetoft \n",
      "Thomas Palmskov \n",
      "Kirstine van Sabben \n",
      "Eik Dahl Bidstrup \n",
      "Dan Arnløv \n",
      "Mads Panny \n",
      "Rabih Azad-Ahmad \n",
      "Sune Raunkjær \n",
      "Arne Lægaard \n",
      "Karsten Uno Petersen \n",
      "Jakob Rixen \n",
      "Ole Revsgaard Andersen \n",
      "Flemming Rømer \n",
      "Mortensiig \n",
      "PeterSkriverNielsen \n",
      "Lars Mogensen \n",
      "Dan Skjerning \n",
      "Knud Holt Nielsen \n",
      "Kristian Skov-Andersen \n",
      "Ursula Dieterich \n",
      "Jørgen Gaarde \n",
      "Kirsten Devantier \n",
      "Peder Udengaard \n",
      "John Brædder \n",
      "Olav Rabølle Nielsen \n",
      "Nils Borring \n",
      "Pia Tørving \n",
      "Ulrich Fredberg \n",
      "vibeke syppli enrum \n",
      "Mathias Frosz \n",
      "Gyda Heding \n",
      "Claus Bakke \n",
      "Cecilie Roed Schultz \n",
      "HolgerSchouRasmussen \n",
      "Søren Windell \n",
      "Aki Høegh-Dam \n",
      "Morten Skovlund \n",
      "Per Bødker Andersen \n",
      "Michele Fejø \n",
      "Nicolaj Bang \n",
      "Anette Mortensen \n",
      "Kenneth Bjerregaard \n",
      "H. Kristian Wollesen \n",
      "Lene Krabbe Dahl \n",
      "Erik Swiatek \n",
      "Mai-Britt Beith \n",
      "Lis Mancini \n",
      "Eva B. Mejnertz \n",
      "Flemming Erichsen \n",
      "Iben Sønderup \n",
      "Thomas. \n",
      "Carsten Kristensen \n",
      "Charlotte Drue \n",
      "Ib Kristensen \n",
      "Liselotte Lynge Jensen \n",
      "Randi Mondorf \n",
      "Jens Meilvang \n",
      "Dan Ingemann Jensen \n",
      "Eea Vestergaard \n",
      "Andreas Keil \n",
      "Else Kayser \n",
      "Lene Linnemann \n",
      "NiclasTuran Kandemir \n",
      "Mira Issa Bloch \n",
      "Simon Højer \n",
      "Ole Vind \n",
      "Lene Kjelgaard  \n",
      "Claus Mørkbak Højrup \n",
      "Thies Mathiasen \n",
      "Steffen Jensen \n",
      "Mette Valbjørn \n",
      "Per Bisgaard \n",
      "Marianne Frederik \n",
      "Anders Buhl-Christen \n",
      "Anny winther \n",
      "Christel Gall \n",
      "Henning Ravn \n",
      "anja daugaard \n",
      "Erik Nielsen \n",
      "Palle Lykke Ravn \n",
      "Dina Oxfeldt \n",
      "malene busk \n",
      "Uwe Jessen \n",
      "Sarah Nørris \n",
      "Annie Hagel \n",
      "Annemarie Knigge \n",
      "Michael Eskamp Witek \n",
      "Charlotte Broman Møl \n",
      "PGraversgaard \n",
      "Anne Møller Ronex \n",
      "Lauge Larsen (A) \n",
      "Åse Kubel Høeg \n",
      "Hans Skou \n",
      "John T. Olsen \n",
      "Erik Nørreby \n",
      "Azra Hasanbegovic \n",
      "Jørgen Hein \n",
      "Jørgen Rørbæk \n",
      "Susanne Lundvald \n",
      "Erik Harbo Larsen \n",
      "Susanne Dyreborg \n",
      "Jan Hendeliowitz \n",
      "Tobias Bøgeskov \n",
      "Henrik Buchhave \n",
      "Flemming Andersen \n",
      "Pernille Høxbro \n",
      "Poul Fremmelev \n",
      "Helle Sjelle \n",
      "Bente Gertz \n",
      "Erik R Gregersen \n",
      "Anders R. Laugesen \n",
      "Flemming Brank \n",
      "Niels Lundshøj (S) \n",
      "karen lagoni \n",
      "Maria Brumvig \n",
      "Bruno Jerup \n",
      "Steen Jakobsen \n",
      "Søren Elbæk \n",
      "Annette Randløv \n",
      "Linda Frederiksen \n",
      "claus leick \n",
      "Mogens Nørgård \n",
      "martin schepelern \n",
      "Per Skovmose \n",
      "Anna Kirsten Olesen  \n",
      "Petter Astrup \n",
      "Kim Drejer Nielsen \n",
      "Annie Hagel \n",
      "Søren H Lambertsen \n",
      "Jakob Ville \n",
      "Maria Temponeras \n",
      "Nikolai Norup \n",
      "Susanne Buch Nielsen \n",
      "Lone Langballe \n",
      "Anja Lund \n",
      "David M. Zepernick \n",
      "Sanne Skougaard An.. \n",
      "Evan Lynnerup \n",
      "Ole Alsted \n",
      "Lene Madsen Milner \n",
      "Tage Petersen \n",
      "Nini Oken \n",
      "Mette Bjerre \n",
      "melanie simick \n",
      "Poul-Erik Svendsen \n",
      "Medzait Ljatifi \n",
      "Niels Hörup \n",
      "Ulla Chambless \n",
      "Poul Erik Jensen \n",
      "Frands Fischer \n",
      "Tage Leegaard \n",
      "Vicky Bender Lorenzen \n",
      "Peder Hummelmose \n",
      "Susan Hedlund \n",
      "Dorte West \n",
      "Mustapha El-Ahmad \n",
      "Arne Kristiansen \n",
      "Finn Sander Jensen \n",
      "Dorthe Hecht \n",
      "Mogens Nyholm \n",
      "Karsten Fogde \n",
      "Hamlaoui Bahloul \n",
      "Jørgen Ahlquist \n",
      "conny Jensen \n",
      "Henrik Vallø \n",
      "Kim Hesel \n",
      "Freja Södergran \n",
      "Birthe Harritz \n",
      "Trine Frengler \n",
      "Michael Harbøll \n",
      "ChristofferReinhardt \n",
      "MortenWeiss-Pedersen \n",
      "Jørn Lehmann Peterse \n",
      "Henning J. Nyhuus \n",
      "Peter Therkildsen \n",
      "Kasper Ejlertsen \n",
      "Jette Ramskov \n",
      "Anne Heeager \n",
      "Karsten SkawboJensen \n",
      "Jan Hemberg Herskov \n",
      "liv gro jensen \n",
      "Anders Korsbæk \n",
      "Søren Erik  Pedersen \n",
      "Leif Olsen \n",
      "Ellen Petersen \n",
      "Felex Pedersen \n",
      "AMPJ \n",
      "Christina Kjærsgaard \n",
      "Molle lykke nielsen \n",
      "Vibeke Gamst \n",
      "Pia Moldt \n",
      "Lars Folmann \n",
      "schlootie \n"
     ]
    }
   ],
   "source": [
    "real_names = soup.find_all('small')\n",
    "for r_names in real_names:\n",
    "    result=re.search(r'<small>.+',str(r_names)).group(0).replace(\"<small>\",\"\")\n",
    "    real_name_list.append(result.rstrip())\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the twittername with their corresponding real names are saved into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'Names':real_name_list,\"Twittername\":twitter_name_list}\n",
    "data=pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv(\"danish_politkere_twitter.csv\") #For saving the file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Twittername</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Margrethe Vestager</td>\n",
       "      <td>vestager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lars Løkke Rasmussen</td>\n",
       "      <td>larsloekke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morten Østergaard</td>\n",
       "      <td>oestergaard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pernille Skipper</td>\n",
       "      <td>PSkipperEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ida Auken</td>\n",
       "      <td>IdaAuken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Names  Twittername\n",
       "0    Margrethe Vestager     vestager\n",
       "1  Lars Løkke Rasmussen   larsloekke\n",
       "2     Morten Østergaard  oestergaard\n",
       "3      Pernille Skipper   PSkipperEL\n",
       "4             Ida Auken     IdaAuken"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be used to easily load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"danish_politkere_twitter.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets (who tags who)  (LAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From twitter the only profiles needed is the politicians which just have been scraped.\n",
    "\n",
    "The twitternames are saved in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_name_list=list(data[\"Twittername\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using the API, both API key, API secret key, access token and access token-secret is needed. For privacy matter, these has been saved in files saved locally. The code below shows how the data are imported from our computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "User='Lau'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load keys (everytime except first)\n",
    "with open('api_key_'+User+'.data', 'rb') as filehandle:\n",
    "    api_key = pickle.load(filehandle) \n",
    "with open('api_key_secret_'+User+'.data', 'rb') as filehandle:\n",
    "    api_secret_key = pickle.load(filehandle)\n",
    "with open('access_token_'+User+'.data', 'rb') as filehandle:\n",
    "    access_token = pickle.load(filehandle)\n",
    "with open('access_token_secret_'+User+'.data', 'rb') as filehandle:\n",
    "    access_token_secret = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For accessing twitter data, the library Tweepy has been used. Since only tweets from the politicians are needed, tweepy is very suitable for doing the job. Here is a link to the library webpage: https://www.tweepy.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the API object which uses [OAuth](https://oauth.net/) for secure authorization. It enables interaction with the twitter API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the authentication object\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "# Setting access token and secret\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "# Creating the API object while passing in auth information\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give a small introduction on how it works, below 5 tweets from the politician Margrethe Vestager is printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @Forsvarsmin: Danmark medvirker til et stærkt og robust digitalt EU. \n",
      "Godt møde med @vestager om cybersikkerhed, hybride trusler,\n",
      "cybers…\n",
      "--------------------------------------------------------------------------------\n",
      "RT @VeraJourova: Today's Media &amp; Audiovisual Action Plan\n",
      "✅supports the recovery, the digital transformation of these sectors, and a level p…\n",
      "--------------------------------------------------------------------------------\n",
      "Time for Europe to set strong rules for the digital world for the next decade\n",
      "Together with Thierry @ThierryBreton we are ready for\n",
      "#DigitalServicesAct\n",
      "#DigitalMarketsAct https://t.co/e4jv7FkhWs\n",
      "--------------------------------------------------------------------------------\n",
      "Working on technology that we can trust, free data flows, fair digital taxation. So much 🇪🇺 and 🇺🇸 can do together! https://t.co/R3jSBSHk6q And a lot of work for our future EU-US Trade &amp; Technology Council w @VDombrovskis Looking forward to working with you @Transition46 team 🤝\n",
      "--------------------------------------------------------------------------------\n",
      "One year already! What a year! Bringing together the urgent need to support Europe's businesses in the #COVID19 crisis and the long-term work for a strong, open #DigitalEU. Looking forward to the years ahead ❤️🇪🇺 #vdLcommission https://t.co/TzRWIxuKkG https://t.co/aA0w7r0NHf\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The Twitter user who we want to get tweets from\n",
    "name = \"vestager\"\n",
    "# Number of tweets to pull\n",
    "tweetCount = 5\n",
    "\n",
    "# Calling the user_timeline function with our parameters\n",
    "tweets = api.user_timeline(id=name, count=tweetCount,tweet_mode='extended')\n",
    "\n",
    "# foreach through all tweets pulled\n",
    "for tweet in tweets:\n",
    "    # printing the text stored inside the tweet object\n",
    "    print(tweet.full_text)\n",
    "    print(80 * '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of dictionary is as follows:\n",
    "\n",
    "\n",
    "* Vestager :\n",
    "       Larslokke : [tweettextA, tweettextB] \n",
    "       oestergaard : [tweettextC] \n",
    "\n",
    "\n",
    "Each key correspond to a politician twitter profiles e.g. Vestager. Vestager has tagged Larslokke in two tweets and oestergaard in one tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict={\"Dummy\":{}} #The dummy is deleted later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving all the twitterprofiles (again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " all_persons=list(data[\"Twittername\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses Tweepy to the last 200 tweets (the limit of the API) of each politician. \n",
    "\n",
    "If politician (P0) tags another politician (P1), then P1 is saved, and a corresponding list with all tweets where P0 mentions P1 is also saved.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 / 651\n",
      "Loading 2 / 651\n",
      "Loading 3 / 651\n",
      "Loading 4 / 651\n",
      "Loading 5 / 651\n",
      "Loading 6 / 651\n",
      "Loading 7 / 651\n",
      "Loading 8 / 651\n",
      "Loading 9 / 651\n",
      "Loading 10 / 651\n",
      "Loading 11 / 651\n",
      "Loading 12 / 651\n",
      "Loading 13 / 651\n",
      "Loading 14 / 651\n",
      "Loading 15 / 651\n",
      "Loading 16 / 651\n",
      "Loading 17 / 651\n",
      "Loading 18 / 651\n",
      "Loading 19 / 651\n",
      "Loading 20 / 651\n",
      "Loading 21 / 651\n",
      "Loading 22 / 651\n",
      "Loading 23 / 651\n",
      "Loading 24 / 651\n",
      "Loading 25 / 651\n",
      "Loading 26 / 651\n",
      "Loading 27 / 651\n",
      "Loading 28 / 651\n",
      "Loading 29 / 651\n",
      "Loading 30 / 651\n",
      "Loading 31 / 651\n",
      "Loading 32 / 651\n",
      "Loading 33 / 651\n",
      "Loading 34 / 651\n",
      "Loading 35 / 651\n",
      "Loading 36 / 651\n",
      "Loading 37 / 651\n",
      "Loading 38 / 651\n",
      "Loading 39 / 651\n",
      "Loading 40 / 651\n",
      "Loading 41 / 651\n",
      "Loading 42 / 651\n",
      "Loading 43 / 651\n",
      "Loading 44 / 651\n",
      "Loading 45 / 651\n",
      "Loading 46 / 651\n",
      "Loading 47 / 651\n",
      "Loading 48 / 651\n",
      "Loading 49 / 651\n",
      "Error at SosseSass\n",
      "Loading 50 / 651\n",
      "Loading 51 / 651\n",
      "Loading 52 / 651\n",
      "Loading 53 / 651\n",
      "Loading 54 / 651\n",
      "Loading 55 / 651\n",
      "Loading 56 / 651\n",
      "Loading 57 / 651\n",
      "Loading 58 / 651\n",
      "Loading 59 / 651\n",
      "Loading 60 / 651\n",
      "Loading 61 / 651\n",
      "Loading 62 / 651\n",
      "Loading 63 / 651\n",
      "Loading 64 / 651\n",
      "Loading 65 / 651\n",
      "Loading 66 / 651\n",
      "Loading 67 / 651\n",
      "Loading 68 / 651\n",
      "Loading 69 / 651\n",
      "Loading 70 / 651\n",
      "Loading 71 / 651\n",
      "Loading 72 / 651\n",
      "Loading 73 / 651\n",
      "Loading 74 / 651\n",
      "Loading 75 / 651\n",
      "Loading 76 / 651\n",
      "Loading 77 / 651\n",
      "Loading 78 / 651\n",
      "Loading 79 / 651\n",
      "Loading 80 / 651\n",
      "Error at MFVMin\n",
      "Loading 81 / 651\n",
      "Loading 82 / 651\n",
      "Loading 83 / 651\n",
      "Loading 84 / 651\n",
      "Loading 85 / 651\n",
      "Loading 86 / 651\n",
      "Loading 87 / 651\n",
      "Loading 88 / 651\n",
      "Loading 89 / 651\n",
      "Loading 90 / 651\n",
      "Loading 91 / 651\n",
      "Loading 92 / 651\n",
      "Loading 93 / 651\n",
      "Loading 94 / 651\n",
      "Loading 95 / 651\n",
      "Loading 96 / 651\n",
      "Loading 97 / 651\n",
      "Loading 98 / 651\n",
      "Loading 99 / 651\n",
      "Loading 100 / 651\n",
      "Loading 101 / 651\n",
      "Loading 102 / 651\n",
      "Loading 103 / 651\n",
      "Loading 104 / 651\n",
      "Loading 105 / 651\n",
      "Loading 106 / 651\n",
      "Loading 107 / 651\n",
      "Loading 108 / 651\n",
      "Loading 109 / 651\n",
      "Loading 110 / 651\n",
      "Loading 111 / 651\n",
      "Loading 112 / 651\n",
      "Loading 113 / 651\n",
      "Loading 114 / 651\n",
      "Loading 115 / 651\n",
      "Loading 116 / 651\n",
      "Loading 117 / 651\n",
      "Loading 118 / 651\n",
      "Loading 119 / 651\n",
      "Loading 120 / 651\n",
      "Loading 121 / 651\n",
      "Loading 122 / 651\n",
      "Loading 123 / 651\n",
      "Loading 124 / 651\n",
      "Loading 125 / 651\n",
      "Loading 126 / 651\n",
      "Loading 127 / 651\n",
      "Loading 128 / 651\n",
      "Loading 129 / 651\n",
      "Loading 130 / 651\n",
      "Loading 131 / 651\n",
      "Loading 132 / 651\n",
      "Loading 133 / 651\n",
      "Loading 134 / 651\n",
      "Error at Simonpihl\n",
      "Loading 135 / 651\n",
      "Loading 136 / 651\n",
      "Loading 137 / 651\n",
      "Loading 138 / 651\n",
      "Loading 139 / 651\n",
      "Loading 140 / 651\n",
      "Loading 141 / 651\n",
      "Loading 142 / 651\n",
      "Loading 143 / 651\n",
      "Loading 144 / 651\n",
      "Loading 145 / 651\n",
      "Loading 146 / 651\n",
      "Loading 147 / 651\n",
      "Loading 148 / 651\n",
      "Loading 149 / 651\n",
      "Loading 150 / 651\n",
      "Loading 151 / 651\n",
      "Loading 152 / 651\n",
      "Loading 153 / 651\n",
      "Loading 154 / 651\n",
      "Loading 155 / 651\n",
      "Loading 156 / 651\n",
      "Loading 157 / 651\n",
      "Loading 158 / 651\n",
      "Loading 159 / 651\n",
      "Loading 160 / 651\n",
      "Loading 161 / 651\n",
      "Loading 162 / 651\n",
      "Loading 163 / 651\n",
      "Loading 164 / 651\n",
      "Loading 165 / 651\n",
      "Loading 166 / 651\n",
      "Loading 167 / 651\n",
      "Loading 168 / 651\n",
      "Loading 169 / 651\n",
      "Loading 170 / 651\n",
      "Loading 171 / 651\n",
      "Loading 172 / 651\n",
      "Loading 173 / 651\n",
      "Loading 174 / 651\n",
      "Loading 175 / 651\n",
      "Loading 176 / 651\n",
      "Loading 177 / 651\n",
      "Loading 178 / 651\n",
      "Loading 179 / 651\n",
      "Loading 180 / 651\n",
      "Loading 181 / 651\n",
      "Loading 182 / 651\n",
      "Loading 183 / 651\n",
      "Loading 184 / 651\n",
      "Loading 185 / 651\n",
      "Loading 186 / 651\n",
      "Loading 187 / 651\n",
      "Loading 188 / 651\n",
      "Loading 189 / 651\n",
      "Loading 190 / 651\n",
      "Loading 191 / 651\n",
      "Loading 192 / 651\n",
      "Loading 193 / 651\n",
      "Loading 194 / 651\n",
      "Loading 195 / 651\n",
      "Loading 196 / 651\n",
      "Loading 197 / 651\n",
      "Loading 198 / 651\n",
      "Loading 199 / 651\n",
      "Loading 200 / 651\n",
      "Loading 201 / 651\n",
      "Loading 202 / 651\n",
      "Loading 203 / 651\n",
      "Loading 204 / 651\n",
      "Loading 205 / 651\n",
      "Loading 206 / 651\n",
      "Loading 207 / 651\n",
      "Loading 208 / 651\n",
      "Loading 209 / 651\n",
      "Loading 210 / 651\n",
      "Loading 211 / 651\n",
      "Loading 212 / 651\n",
      "Loading 213 / 651\n",
      "Loading 214 / 651\n",
      "Error at GeertsenVenstre\n",
      "Loading 215 / 651\n",
      "Loading 216 / 651\n",
      "Loading 217 / 651\n",
      "Loading 218 / 651\n",
      "Loading 219 / 651\n",
      "Loading 220 / 651\n",
      "Error at BendixenPebe\n",
      "Loading 221 / 651\n",
      "Loading 222 / 651\n",
      "Loading 223 / 651\n",
      "Loading 224 / 651\n",
      "Loading 225 / 651\n",
      "Loading 226 / 651\n",
      "Loading 227 / 651\n",
      "Loading 228 / 651\n",
      "Loading 229 / 651\n",
      "Loading 230 / 651\n",
      "Loading 231 / 651\n",
      "Loading 232 / 651\n",
      "Loading 233 / 651\n",
      "Loading 234 / 651\n",
      "Loading 235 / 651\n",
      "Error at AsgerChristens2\n",
      "Loading 236 / 651\n",
      "Loading 237 / 651\n",
      "Loading 238 / 651\n",
      "Loading 239 / 651\n",
      "Loading 240 / 651\n",
      "Loading 241 / 651\n",
      "Loading 242 / 651\n",
      "Loading 243 / 651\n",
      "Loading 244 / 651\n",
      "Loading 245 / 651\n",
      "Loading 246 / 651\n",
      "Loading 247 / 651\n",
      "Loading 248 / 651\n",
      "Loading 249 / 651\n",
      "Loading 250 / 651\n",
      "Loading 251 / 651\n",
      "Loading 252 / 651\n",
      "Loading 253 / 651\n",
      "Loading 254 / 651\n",
      "Loading 255 / 651\n",
      "Loading 256 / 651\n",
      "Loading 257 / 651\n",
      "Loading 258 / 651\n",
      "Loading 259 / 651\n",
      "Loading 260 / 651\n",
      "Loading 261 / 651\n",
      "Loading 262 / 651\n",
      "Loading 263 / 651\n",
      "Loading 264 / 651\n",
      "Loading 265 / 651\n",
      "Loading 266 / 651\n",
      "Loading 267 / 651\n",
      "Loading 268 / 651\n",
      "Loading 269 / 651\n",
      "Loading 270 / 651\n",
      "Loading 271 / 651\n",
      "Loading 272 / 651\n",
      "Loading 273 / 651\n",
      "Loading 274 / 651\n",
      "Loading 275 / 651\n",
      "Loading 276 / 651\n",
      "Loading 277 / 651\n",
      "Loading 278 / 651\n",
      "Loading 279 / 651\n",
      "Loading 280 / 651\n",
      "Loading 281 / 651\n",
      "Loading 282 / 651\n",
      "Loading 283 / 651\n",
      "Loading 284 / 651\n",
      "Loading 285 / 651\n",
      "Error at ToftJakobsen\n",
      "Loading 286 / 651\n",
      "Loading 287 / 651\n",
      "Loading 288 / 651\n",
      "Loading 289 / 651\n",
      "Loading 290 / 651\n",
      "Loading 291 / 651\n",
      "Loading 292 / 651\n",
      "Loading 293 / 651\n",
      "Loading 294 / 651\n",
      "Loading 295 / 651\n",
      "Loading 296 / 651\n",
      "Loading 297 / 651\n",
      "Loading 298 / 651\n",
      "Loading 299 / 651\n",
      "Loading 300 / 651\n",
      "Loading 301 / 651\n",
      "Loading 302 / 651\n",
      "Loading 303 / 651\n",
      "Error at simonsjsimonsen\n",
      "Loading 304 / 651\n",
      "Loading 305 / 651\n",
      "Loading 306 / 651\n",
      "Loading 307 / 651\n",
      "Loading 308 / 651\n",
      "Loading 309 / 651\n",
      "Loading 310 / 651\n",
      "Loading 311 / 651\n",
      "Loading 312 / 651\n",
      "Loading 313 / 651\n",
      "Loading 314 / 651\n",
      "Loading 315 / 651\n",
      "Loading 316 / 651\n",
      "Loading 317 / 651\n",
      "Loading 318 / 651\n",
      "Loading 319 / 651\n",
      "Loading 320 / 651\n",
      "Loading 321 / 651\n",
      "Loading 322 / 651\n",
      "Loading 323 / 651\n",
      "Loading 324 / 651\n",
      "Loading 325 / 651\n",
      "Error at Latifa_Venstre\n",
      "Loading 326 / 651\n",
      "Loading 327 / 651\n",
      "Loading 328 / 651\n",
      "Loading 329 / 651\n",
      "Loading 330 / 651\n",
      "Loading 331 / 651\n",
      "Loading 332 / 651\n",
      "Loading 333 / 651\n",
      "Error at GlenMadsen\n",
      "Loading 334 / 651\n",
      "Loading 335 / 651\n",
      "Loading 336 / 651\n",
      "Loading 337 / 651\n",
      "Loading 338 / 651\n",
      "Loading 339 / 651\n",
      "Loading 340 / 651\n",
      "Loading 341 / 651\n",
      "Loading 342 / 651\n",
      "Loading 343 / 651\n",
      "Loading 344 / 651\n",
      "Loading 345 / 651\n",
      "Loading 346 / 651\n",
      "Loading 347 / 651\n",
      "Loading 348 / 651\n",
      "Loading 349 / 651\n",
      "Loading 350 / 651\n",
      "Loading 351 / 651\n",
      "Loading 352 / 651\n",
      "Loading 353 / 651\n",
      "Loading 354 / 651\n",
      "Loading 355 / 651\n",
      "Loading 356 / 651\n",
      "Loading 357 / 651\n",
      "Loading 358 / 651\n",
      "Loading 359 / 651\n",
      "Loading 360 / 651\n",
      "Loading 361 / 651\n",
      "Loading 362 / 651\n",
      "Loading 363 / 651\n",
      "Loading 364 / 651\n",
      "Loading 365 / 651\n",
      "Loading 366 / 651\n",
      "Loading 367 / 651\n",
      "Loading 368 / 651\n",
      "Loading 369 / 651\n",
      "Loading 370 / 651\n",
      "Loading 371 / 651\n",
      "Loading 372 / 651\n",
      "Loading 373 / 651\n",
      "Loading 374 / 651\n",
      "Loading 375 / 651\n",
      "Loading 376 / 651\n",
      "Loading 377 / 651\n",
      "Loading 378 / 651\n",
      "Loading 379 / 651\n",
      "Loading 380 / 651\n",
      "Loading 381 / 651\n",
      "Loading 382 / 651\n",
      "Loading 383 / 651\n",
      "Loading 384 / 651\n",
      "Loading 385 / 651\n",
      "Loading 386 / 651\n",
      "Loading 387 / 651\n",
      "Loading 388 / 651\n",
      "Loading 389 / 651\n",
      "Loading 390 / 651\n",
      "Loading 391 / 651\n",
      "Loading 392 / 651\n",
      "Loading 393 / 651\n",
      "Loading 394 / 651\n",
      "Loading 395 / 651\n",
      "Loading 396 / 651\n",
      "Loading 397 / 651\n",
      "Loading 398 / 651\n",
      "Loading 399 / 651\n",
      "Loading 400 / 651\n",
      "Loading 401 / 651\n",
      "Loading 402 / 651\n",
      "Loading 403 / 651\n",
      "Loading 404 / 651\n",
      "Loading 405 / 651\n",
      "Loading 406 / 651\n",
      "Loading 407 / 651\n",
      "Loading 408 / 651\n",
      "Loading 409 / 651\n",
      "Loading 410 / 651\n",
      "Loading 411 / 651\n",
      "Loading 412 / 651\n",
      "Loading 413 / 651\n",
      "Loading 414 / 651\n",
      "Loading 415 / 651\n",
      "Loading 416 / 651\n",
      "Loading 417 / 651\n",
      "Loading 418 / 651\n",
      "Loading 419 / 651\n",
      "Loading 420 / 651\n",
      "Loading 421 / 651\n",
      "Loading 422 / 651\n",
      "Loading 423 / 651\n",
      "Loading 424 / 651\n",
      "Loading 425 / 651\n",
      "Loading 426 / 651\n",
      "Loading 427 / 651\n",
      "Loading 428 / 651\n",
      "Loading 429 / 651\n",
      "Loading 430 / 651\n",
      "Loading 431 / 651\n",
      "Loading 432 / 651\n",
      "Loading 433 / 651\n",
      "Loading 434 / 651\n",
      "Loading 435 / 651\n",
      "Loading 436 / 651\n",
      "Loading 437 / 651\n",
      "Loading 438 / 651\n",
      "Loading 439 / 651\n",
      "Loading 440 / 651\n",
      "Loading 441 / 651\n",
      "Loading 442 / 651\n",
      "Loading 443 / 651\n",
      "Loading 444 / 651\n",
      "Loading 445 / 651\n",
      "Loading 446 / 651\n",
      "Loading 447 / 651\n",
      "Loading 448 / 651\n",
      "Loading 449 / 651\n",
      "Loading 450 / 651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 451 / 651\n",
      "Loading 452 / 651\n",
      "Loading 453 / 651\n",
      "Loading 454 / 651\n",
      "Loading 455 / 651\n",
      "Loading 456 / 651\n",
      "Loading 457 / 651\n",
      "Loading 458 / 651\n",
      "Loading 459 / 651\n",
      "Loading 460 / 651\n",
      "Loading 461 / 651\n",
      "Loading 462 / 651\n",
      "Loading 463 / 651\n",
      "Loading 464 / 651\n",
      "Loading 465 / 651\n",
      "Loading 466 / 651\n",
      "Loading 467 / 651\n",
      "Loading 468 / 651\n",
      "Loading 469 / 651\n",
      "Loading 470 / 651\n",
      "Error at flemse68\n",
      "Loading 471 / 651\n",
      "Loading 472 / 651\n",
      "Loading 473 / 651\n",
      "Loading 474 / 651\n",
      "Loading 475 / 651\n",
      "Loading 476 / 651\n",
      "Loading 477 / 651\n",
      "Loading 478 / 651\n",
      "Loading 479 / 651\n",
      "Loading 480 / 651\n",
      "Loading 481 / 651\n",
      "Loading 482 / 651\n",
      "Loading 483 / 651\n",
      "Loading 484 / 651\n",
      "Loading 485 / 651\n",
      "Loading 486 / 651\n",
      "Loading 487 / 651\n",
      "Loading 488 / 651\n",
      "Loading 489 / 651\n",
      "Loading 490 / 651\n",
      "Loading 491 / 651\n",
      "Loading 492 / 651\n",
      "Loading 493 / 651\n",
      "Loading 494 / 651\n",
      "Loading 495 / 651\n",
      "Loading 496 / 651\n",
      "Loading 497 / 651\n",
      "Loading 498 / 651\n",
      "Loading 499 / 651\n",
      "Loading 500 / 651\n",
      "Loading 501 / 651\n",
      "Loading 502 / 651\n",
      "Loading 503 / 651\n",
      "Loading 504 / 651\n",
      "Loading 505 / 651\n",
      "Loading 506 / 651\n",
      "Loading 507 / 651\n",
      "Loading 508 / 651\n",
      "Loading 509 / 651\n",
      "Loading 510 / 651\n",
      "Loading 511 / 651\n",
      "Loading 512 / 651\n",
      "Loading 513 / 651\n",
      "Loading 514 / 651\n",
      "Loading 515 / 651\n",
      "Loading 516 / 651\n",
      "Loading 517 / 651\n",
      "Loading 518 / 651\n",
      "Loading 519 / 651\n",
      "Loading 520 / 651\n",
      "Loading 521 / 651\n",
      "Loading 522 / 651\n",
      "Loading 523 / 651\n",
      "Loading 524 / 651\n",
      "Loading 525 / 651\n",
      "Loading 526 / 651\n",
      "Loading 527 / 651\n",
      "Loading 528 / 651\n",
      "Loading 529 / 651\n",
      "Loading 530 / 651\n",
      "Loading 531 / 651\n",
      "Loading 532 / 651\n",
      "Loading 533 / 651\n",
      "Loading 534 / 651\n",
      "Loading 535 / 651\n",
      "Loading 536 / 651\n",
      "Loading 537 / 651\n",
      "Loading 538 / 651\n",
      "Loading 539 / 651\n",
      "Loading 540 / 651\n",
      "Loading 541 / 651\n",
      "Loading 542 / 651\n",
      "Loading 543 / 651\n",
      "Loading 544 / 651\n",
      "Loading 545 / 651\n",
      "Loading 546 / 651\n",
      "Loading 547 / 651\n",
      "Loading 548 / 651\n",
      "Loading 549 / 651\n",
      "Loading 550 / 651\n",
      "Loading 551 / 651\n",
      "Loading 552 / 651\n",
      "Loading 553 / 651\n",
      "Loading 554 / 651\n",
      "Loading 555 / 651\n",
      "Loading 556 / 651\n",
      "Loading 557 / 651\n",
      "Loading 558 / 651\n",
      "Loading 559 / 651\n",
      "Loading 560 / 651\n",
      "Loading 561 / 651\n",
      "Loading 562 / 651\n",
      "Loading 563 / 651\n",
      "Loading 564 / 651\n",
      "Loading 565 / 651\n",
      "Loading 566 / 651\n",
      "Loading 567 / 651\n",
      "Loading 568 / 651\n",
      "Loading 569 / 651\n",
      "Loading 570 / 651\n",
      "Loading 571 / 651\n",
      "Loading 572 / 651\n",
      "Loading 573 / 651\n",
      "Loading 574 / 651\n",
      "Loading 575 / 651\n",
      "Loading 576 / 651\n",
      "Loading 577 / 651\n",
      "Loading 578 / 651\n",
      "Loading 579 / 651\n",
      "Loading 580 / 651\n",
      "Loading 581 / 651\n",
      "Loading 582 / 651\n",
      "Loading 583 / 651\n",
      "Loading 584 / 651\n",
      "Loading 585 / 651\n",
      "Loading 586 / 651\n",
      "Loading 587 / 651\n",
      "Loading 588 / 651\n",
      "Loading 589 / 651\n",
      "Loading 590 / 651\n",
      "Loading 591 / 651\n",
      "Loading 592 / 651\n",
      "Loading 593 / 651\n",
      "Loading 594 / 651\n",
      "Loading 595 / 651\n",
      "Loading 596 / 651\n",
      "Loading 597 / 651\n",
      "Loading 598 / 651\n",
      "Loading 599 / 651\n",
      "Loading 600 / 651\n",
      "Loading 601 / 651\n",
      "Loading 602 / 651\n",
      "Loading 603 / 651\n",
      "Loading 604 / 651\n",
      "Loading 605 / 651\n",
      "Loading 606 / 651\n",
      "Loading 607 / 651\n",
      "Loading 608 / 651\n",
      "Loading 609 / 651\n",
      "Loading 610 / 651\n",
      "Loading 611 / 651\n",
      "Loading 612 / 651\n",
      "Loading 613 / 651\n",
      "Loading 614 / 651\n",
      "Loading 615 / 651\n",
      "Loading 616 / 651\n",
      "Loading 617 / 651\n",
      "Loading 618 / 651\n",
      "Loading 619 / 651\n",
      "Loading 620 / 651\n",
      "Loading 621 / 651\n",
      "Loading 622 / 651\n",
      "Loading 623 / 651\n",
      "Loading 624 / 651\n",
      "Loading 625 / 651\n",
      "Loading 626 / 651\n",
      "Loading 627 / 651\n",
      "Loading 628 / 651\n",
      "Loading 629 / 651\n",
      "Loading 630 / 651\n",
      "Loading 631 / 651\n",
      "Error at rn_lehmann\n",
      "Loading 632 / 651\n",
      "Loading 633 / 651\n",
      "Loading 634 / 651\n",
      "Loading 635 / 651\n",
      "Loading 636 / 651\n",
      "Loading 637 / 651\n",
      "Loading 638 / 651\n",
      "Loading 639 / 651\n",
      "Loading 640 / 651\n",
      "Loading 641 / 651\n",
      "Loading 642 / 651\n",
      "Loading 643 / 651\n",
      "Loading 644 / 651\n",
      "Loading 645 / 651\n",
      "Loading 646 / 651\n",
      "Loading 647 / 651\n",
      "Loading 648 / 651\n",
      "Loading 649 / 651\n",
      "Loading 650 / 651\n",
      "Loading 651 / 651\n"
     ]
    }
   ],
   "source": [
    "n_count=0\n",
    "n_total=len(all_persons)\n",
    "for p in all_persons:   \n",
    "    n_count=n_count+1\n",
    "    print(\"Loading {} / {}\".format(n_count,n_total))\n",
    "\n",
    "\n",
    "    # The Twitter user who we want to get tweets from\n",
    "    name = p\n",
    "    # Number of tweets to pull\n",
    "    tweetCount = 200\n",
    "\n",
    "    #Some of the twitterprofiles may have changed twittername or deleted the profile. Therefore try/except.\n",
    "    try:\n",
    "        # Calling the user_timeline function with our parameters\n",
    "        tweets = api.user_timeline(id=name, count=tweetCount,tweet_mode='extended')\n",
    "\n",
    "        if name not in list(tweet_dict.keys()):\n",
    "            ##Updating dict with the new politician if it is not there\n",
    "            tweet_dict.update({name:{}})\n",
    "\n",
    "\n",
    "        ###############========== LOOPING THROUGH ALL THE TWEET FOR THE POLITICIAN=============#####\n",
    "\n",
    "        for i,tweet in enumerate(tweets):\n",
    "    \n",
    "                #Save the text\n",
    "                opslag=tweet.full_text\n",
    "                #Find all tagging\n",
    "                match=re.findall(r\"@\\w+\",opslag)\n",
    "\n",
    "\n",
    "                if len(match)>0:\n",
    "                    \n",
    "                    #Loop through all tagged politican in the tweet\n",
    "                    for a in range(len(match)):\n",
    "                        #Remove the '@'\n",
    "                        uden_snabela=str(match[a]).replace(\"@\",\"\")\n",
    "                        \n",
    "                        #If the twittername is one of the politicians then it should be saved\n",
    "                        if uden_snabela in twitter_name_list:\n",
    "                            \n",
    "                            #If the politician P0, has not mentioned politician P1 before\n",
    "                            if uden_snabela not in list(tweet_dict[name].keys()):\n",
    "                                tweet_dict[name].update({uden_snabela:[opslag]})\n",
    "                            #If the politician P0, HAS mentioned politician P1 before, the list is appended\n",
    "                            else:\n",
    "                                tweet_dict[name][uden_snabela].append(opslag)\n",
    "    except:\n",
    "        print(\"Error at {}\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the dummy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweet_dict[\"Dummy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving all data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('tweet_dict_listversion.data', 'wb') as filehandle:\n",
    "#    # store the data as binary data stream\n",
    "#    pickle.dump(tweet_dict, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the politicians could perhaps mention eachother without using the tag (@). Therefore, the amount of data is increased by including mentioning of politicians full names. Here's an example:\n",
    "\n",
    "vestager tweets: \"Morten Østergaard try to read this post, how (...)\"\n",
    "\n",
    "Morten Østergaard's twittername is oestergaard, and therefore, this tweet should be related to the twitterprofile \"oestergaard\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To attach the tweet to the corresponding twittername, the twitternames are saved in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_name_list=list(data[\"Twittername\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real names in the data is a combinaed of low letters and capital letters. All names are then lower-cased, and saved in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Names\"]=data[\"Names\"].str.lower() ##LAVER DEM TIL LOWER\n",
    "names_list=list(data[\"Names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Morten Østergaards twittername shall be found, it can be done like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    oestergaard\n",
       "Name: Twittername, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data[\"Names\"]==\"morten østergaard\"][\"Twittername\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the tweets of the politicians are then revised, and the following code adds the tweets which has a real name included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'three_persons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-2ce030649eb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mn_total\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthree_persons\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthree_persons\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mn_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_count\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loading {} / {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_total\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'three_persons' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "n_count=0\n",
    "n_total=len(three_persons)\n",
    "for p in three_persons:   \n",
    "    n_count=n_count+1\n",
    "    print(\"Loading {} / {}\".format(n_count,n_total))\n",
    "\n",
    "\n",
    "    # The Twitter user who we want to get tweets from\n",
    "    name = p\n",
    "    # Number of tweets to pull\n",
    "    tweetCount = 200\n",
    "\n",
    "    try:\n",
    "        # Calling the user_timeline function with our parameters\n",
    "        tweets = api.user_timeline(id=name, count=tweetCount,tweet_mode='extended')\n",
    "\n",
    "        ##Updating dict, if the politician has (by some reason) been \"visited\" before\n",
    "        if name not in list(tweet_dict.keys()):\n",
    "            tweet_dict.update({name:{}})\n",
    "\n",
    "\n",
    "\n",
    "        ###############========== FINDING TWEETS FOR PERSON=============#####\n",
    "\n",
    "        for i,tweet in enumerate(tweets):\n",
    "        \n",
    "                #Save the tweet, but this time lowering the text\n",
    "                opslag=tweet.full_text.lower()\n",
    "\n",
    "\n",
    "                #Loop through all real names for the 650 politicians\n",
    "                for real_names in names_list:\n",
    "                    \n",
    "                    if real_names in opslag:\n",
    "                        twittername=data[data[\"Names\"]==real_names][\"Twittername\"] #Find the persons twittername\n",
    "                        twittername=list(twittername)[0] #\"hack\" for removing pandas \"objecttype\"\n",
    "\n",
    "\n",
    "                        #If the politician P0, has not mentioned politician P1 before\n",
    "                        if twittername not in list(tweet_dict[name].keys()):\n",
    "                            tweet_dict[name].update({twittername:[opslag]}) \n",
    "                        else:\n",
    "                            #If the politician P0, HAS mentioned politician P1 before, the list is appended\n",
    "                            tweet_dict[name][twittername].append(opslag)\n",
    "    except:\n",
    "        print(\"Error at {}\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the dat locally\n",
    "#with open('tweet_dict_listversion_wrealnames.data', 'wb') as filehandle:\n",
    "#    pickle.dump(tweet_dict, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary called \"tweet_dict\" now contains data on (almost) all the politicians and who they have been tagging including the assosicated tweets.\n",
    "\n",
    "During the datapreparation, some of the API calls of the twitterprofiles failed. These were:\n",
    "\n",
    "* SosseSass -> He has changed name to SassLarsen \n",
    "* Simonpihl -> He has a private twitterprofile\n",
    "* GeertsenVenstre -> He has changed name to Martin_geertsen\n",
    "* BendixenPebe -> She has maybe deleted/deactivated her profile. No new profile has been found.\n",
    "* ToftJakobsen -> He has maybe deleted/deactivated his profile. No new profile has been found.\n",
    "* simonsjsimonsen -> He has maybe deleted/deactivated his profile. No new profile has been found.\n",
    "* Latifa_Venstre -> She has maybe deleted/deactivated her profile. No new profile has been found.\n",
    "* GlenMadsen -> He has maybe deleted/deactivated his profile. No new profile has been found.\n",
    "* flemse68 -> He has a private twitterprofile\n",
    "* SkovAndersen -> He has changed name to KristianSkov21\n",
    "* rn_lehmann -> He has maybe deleted/deactivated his profile. No new profile has been found.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SosseSass, GeertsenVenstre and SkovAndersen is handled by doing the following in the dataset:\n",
    "\n",
    "* SosseSass is changed to SassLarsen.\n",
    "* GeertsenVenstre is changed to Martin_geertsen\n",
    "* SkovAndersen is changed to KristianSkov21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Twittername']==\"SosseSass\",\"Twittername\"]=\"SassLarsen\"\n",
    "data.loc[data['Twittername']==\"GeertsenVenstre\",\"Twittername\"]=\"Martin_geertsen\"\n",
    "data.loc[data['Twittername']==\"SkovAndersen\",\"Twittername\"]=\"KristianSkov21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_politicians=[\"SassLarsen\",\"Martin_geertsen\",\"KristianSkov21\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All instances of the three politicians twitter names in the dictionary should be updated too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_dict_listversion_wrealnames.data', 'rb') as filehandle:\n",
    "    tweet_dict = pickle.load(filehandle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in list(tweet_dict.keys()):\n",
    "    for elem2 in list(tweet_dict[elem].keys()):            \n",
    "        if elem2==\"SosseSass\":\n",
    "            tweet_dict[elem][\"SassLarsen\"] = tweet_dict[elem].pop(\"SosseSass\")\n",
    "        if elem2==\"GeertsenVenstre\":\n",
    "            tweet_dict[elem][\"Martin_geertsen\"] = tweet_dict[elem].pop(\"GeertsenVenstre\")\n",
    "        if elem2==\"SkovAndersen\":\n",
    "            tweet_dict[elem][\"KristianSkov21\"] = tweet_dict[elem].pop(\"SkovAndersen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('tweet_dict_listversion_wrealnames.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "#    pickle.dump(tweet_dict, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the code for collecting tweets are runned again, but only focusing on the three politicians above. (The new implmentation is at codeline 40):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_count=0\n",
    "n_total=len(all_persons)\n",
    "for p in all_persons not in three_politicians:   \n",
    "    n_count=n_count+1\n",
    "    print(\"Loading {} / {}\".format(n_count,n_total))\n",
    "\n",
    "\n",
    "    # The Twitter user who we want to get tweets from\n",
    "    name = p\n",
    "    # Number of tweets to pull\n",
    "    tweetCount = 200\n",
    "\n",
    "    #Some of the twitterprofiles may have changed twittername or deleted the profile. Therefore try/except.\n",
    "    try:\n",
    "        # Calling the user_timeline function with our parameters\n",
    "        tweets = api.user_timeline(id=name, count=tweetCount,tweet_mode='extended')\n",
    "\n",
    "        ##Updating dict with the new politician\n",
    "        if name not in list(tweet_dict.keys()):\n",
    "            tweet_dict.update({name:{}})\n",
    "\n",
    "\n",
    "        ###############========== LOOPING THROUGH ALL THE TWEET FOR THE POLITICIAN=============#####\n",
    "\n",
    "        for i,tweet in enumerate(tweets):\n",
    "    \n",
    "                #Save the text\n",
    "                opslag=tweet.full_text\n",
    "                #Find all tagging\n",
    "                match=re.findall(r\"@\\w+\",opslag)\n",
    "\n",
    "\n",
    "                if len(match)>0:\n",
    "                    \n",
    "                    #Loop through all tagged politican in the tweet\n",
    "                    for a in range(len(match)):\n",
    "                        #Remove the '@'\n",
    "                        uden_snabela=str(match[a]).replace(\"@\",\"\")\n",
    "                        \n",
    "                        #If the twittername is one of the politicians then it should be saved\n",
    "                        if uden_snabela in three_politicians:\n",
    "                            \n",
    "                            #If the politician P0, has not mentioned politician P1 before\n",
    "                            if uden_snabela not in list(tweet_dict[name].keys()):\n",
    "                                tweet_dict[name].update({uden_snabela:[opslag]})\n",
    "                            #If the politician P0, HAS mentioned politician P1 before, the list is appended\n",
    "                            else:\n",
    "                                tweet_dict[name][uden_snabela].append(opslag)\n",
    "    except:\n",
    "        print(\"Error at {}\".format(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also adding all the politicians who the three politicians has mentioned from their new profiles (new implmentation at code line 3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_count=0\n",
    "n_total=len(three_politicians)\n",
    "for p in three_politicians:   \n",
    "    n_count=n_count+1\n",
    "    print(\"Loading {} / {}\".format(n_count,n_total))\n",
    "\n",
    "\n",
    "    # The Twitter user who we want to get tweets from\n",
    "    name = p\n",
    "    # Number of tweets to pull\n",
    "    tweetCount = 200\n",
    "\n",
    "    #Some of the twitterprofiles may have changed twittername or deleted the profile. Therefore try/except.\n",
    "    try:\n",
    "        # Calling the user_timeline function with our parameters\n",
    "        tweets = api.user_timeline(id=name, count=tweetCount,tweet_mode='extended')\n",
    "\n",
    "        ##Updating dict with the new politician\n",
    "        if name not in list(tweet_dict.keys()):\n",
    "            tweet_dict.update({name:{}})\n",
    "\n",
    "\n",
    "        ###############========== LOOPING THROUGH ALL THE TWEET FOR THE POLITICIAN=============#####\n",
    "\n",
    "        for i,tweet in enumerate(tweets):\n",
    "\n",
    "                #Save the text\n",
    "                opslag=tweet.full_text\n",
    "                #Find all tagging\n",
    "                match=re.findall(r\"@\\w+\",opslag)\n",
    "\n",
    "\n",
    "                if len(match)>0:\n",
    "\n",
    "                    #Loop through all tagged politican in the tweet\n",
    "                    for a in range(len(match)):\n",
    "                        #Remove the '@'\n",
    "                        uden_snabela=str(match[a]).replace(\"@\",\"\")\n",
    "\n",
    "                        #If the twittername is one of the politicians then it should be saved\n",
    "                        if uden_snabela in all_persons:\n",
    "\n",
    "                            #If the politician P0, has not mentioned politician P1 before\n",
    "                            if uden_snabela not in list(tweet_dict[name].keys()):\n",
    "                                tweet_dict[name].update({uden_snabela:[opslag]})\n",
    "                            #If the politician P0, HAS mentioned politician P1 before, the list is appended\n",
    "                            else:\n",
    "                                tweet_dict[name][uden_snabela].append(opslag)\n",
    "    except:\n",
    "        print(\"Error at {}\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('tweet_dict_listversion_wrealnames_v2.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "#    pickle.dump(tweet_dict, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conlusion:** The dictionary called \"tweet_dict\" now contains data on all the politicians which the API could asses. Furthermore the dictionary contains information about who they have been tagging including the associated tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets (only the text)  (LAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To faciliate the topic modelling analysis and wordcloud generation, all of the last 200 tweets from each of the politicians is saved in a dictionary. The keys are politician twitternames and the values are a list of all the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_dict_onlytext={\"Dummy\":{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_count=0\n",
    "n_total=len(all_persons)\n",
    "time_vec=[] #For saving all dates for the tweets\n",
    "for p in all_persons:   \n",
    "    n_count=n_count+1\n",
    "    print(\"Loading {} / {}\".format(n_count,n_total))\n",
    "\n",
    "    # The Twitter user who we want to get tweets from\n",
    "    name = p\n",
    "    # Number of tweets to pull\n",
    "    tweetCount = 200\n",
    "\n",
    "    try:\n",
    "        # Calling the user_timeline function with our parameters\n",
    "        tweets = api.user_timeline(id=name, count=tweetCount,tweet_mode='extended')\n",
    "\n",
    "        ##Updating dict\n",
    "        tweet_dict_onlytext.update({name:{}})\n",
    "\n",
    "        ###############========== FINDING TWEETS FOR PERSON=============#####\n",
    "        for i,tweet in enumerate(tweets):\n",
    "            \n",
    "            time_vec.append(tweet.created_at) #Save all times of the tweets\n",
    "            \n",
    "            opslag=tweet.full_text\n",
    "            if i==0:\n",
    "                tweet_dict_onlytext[name]=[opslag]\n",
    "            else:\n",
    "                tweet_dict_onlytext[name].append(opslag)\n",
    "    except:\n",
    "        print(\"Error at {}\".format(name))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweet_dict[\"Dummy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('time_vec.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    " #   pickle.dump(time_vec, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('tweet_dict_all_v2.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "#    pickle.dump(tweet_dict_onlytext, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out what meetings that should later be used for topic modelling, the time of the tweets are used. The meetings from the parliament should be at the (almost) same time as the most of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vec=[]\n",
    "with open('time_vec.data', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    time_vec=pickle.load( filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_list=[n.year for n in time_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=np.array(year_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(arr,0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr)\n",
    "\n",
    "from scipy import stats\n",
    "100-stats.percentileofscore(df,2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100-stats.percentileofscore(df,2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** All of the politicians respectively 200 tweets are saved in a dictionary \"tweet_dict_onlytext\". When making topic modelling on Parliament meetings later, we need meetings from 2018 to 2020. 71% of the tweets is from this time period. Only 7% of the tweets would be included the time period was expanded to 2017 - BUT, this would add Parliament meeting for a whole year. It was then concluded to not include 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politicians (parties and regions) (CHRISTIAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web scraping galore\n",
    "\n",
    "We quickly discovered that finding a general database for danish politicans and their respective parties was a lot more difficult than what was initially thought. Since, we started with a database over twitter profile, we had to do some digging to find out more about the politician behind them.\n",
    "\n",
    "Firstly we scraped https://www.danskepolitikere.dk/ as it was advertised to be a general database over danish politicians. However, it only consisted of current and old folketing and eropean parliament members. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "data = pd.read_csv(\"danish_politkere_twitter.csv\")\n",
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "soup = BeautifulSoup(requests.get('https://www.danskepolitikere.dk/oversigt/danske-politikere').content, 'html.parser')\n",
    "sections = list(soup.find_all('section'))\n",
    "                     \n",
    "# add \"UNKNOWN\" as default party\n",
    "data['Party'] = 'UNKNOWN'\n",
    "parties = []\n",
    "# cross reference for our politicians and check if he's in the party\n",
    "for row, coloumns in data.iterrows(): \n",
    "    name = coloumns[0] \n",
    "    for section in sections:\n",
    "        party = section.h3.a.text # gets the party \n",
    "        if re.search(name.lower(), str(section).lower()):\n",
    "            data.at[row, 'Party'] = party\n",
    "\n",
    "# view results\n",
    "data.groupby('Party').count()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observered, there's still far too many unknowns, so we had to do some digging. By manual labour, we found out that many of the politicians in this twitter database were smaller politicians outside folketinget.\n",
    "\n",
    "So we scrabed the regions homepages and also folketinget's homepage, the scraping progress is quite tedious and therefore will just be added to the appendix. But essentially, we found that a LARGE majority of politicians are small time, inside their local \"kommune\" and since there are over 90 of these in denmark. We couldn't eliminate all of the unknowns so far.\n",
    "\n",
    "Here's what we ended up with...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=pd.read_csv(\"danish_politkere_twitter2.csv\")\n",
    "data2.groupby('Party').count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Political meetings (FREDERIK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text statistics (twitter and meetings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter text statistics (who mentions who) (LAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_dict_listversion_wrealnames_v2.data', 'rb') as filehandle:\n",
    "    tweet_dict = pickle.load(filehandle) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas dataframe to get an easier overview of the politicians and their associated mentioned politicians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_tweet_dict=pd.DataFrame.from_dict(tweet_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposing the dataset, so the politician who has made the tweet is represented in the rows - the ones who are mentioned are represented in coloumns. NaNs in place (i,j) means that the politician *i* does not mention politician *j*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_tweet_dict=pd_tweet_dict.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_tweet_dict.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of Twitter profiles who is found on twitter: {}\".format(pd_tweet_dict.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of Twitter profiles who is mentioned: {}\".format(pd_tweet_dict.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of distinct links between politicians: {}\".format(pd_tweet_dict.count().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of no-links from a poltician to another: {}\".format(pd_tweet_dict.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 310412 combinations of politicians where theres no mentioning on twitter. This indicates a very sparse matrix. When later using this as a basis for a graph, one would know that it is not a complete graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following piece of code revise the dictionary again and calculates:\n",
    "* The total number of tweets accross all politicians (non-distinct, because multiple politician can be mentioned in same tweet)\n",
    "* Total length of all the tweets\n",
    "* A list for every politician, indicating the length of the each tweets where they mentions other politicians\n",
    "* A list for every politician, indicating the total length all tweets where they mentions other politicians\n",
    "* A number for every politician, indicating the total number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_n_tweets=0\n",
    "total_string_length=0\n",
    "\n",
    "n_tweet_dict=dict()\n",
    "tweet_lengths_all_dict=dict()\n",
    "tweet_length_agg_dict=dict()\n",
    "\n",
    "for pol in list(tweet_dict.keys()):\n",
    "    \n",
    "    tweet_lengths_all_dict[pol]=[]\n",
    "    \n",
    "    n_tweet_dict[pol]=0\n",
    "    \n",
    "    for to_pol in list(tweet_dict[pol].keys()):\n",
    "        elems=tweet_dict[pol][to_pol]\n",
    "        \n",
    "        #sum over the number of words for each tweet\n",
    "        elems_splittet_to_words=[elems[i].split() for i in range(len(elems))]\n",
    "        total_string_length+=sum( map(len,elems_splittet_to_words  )) #Summing the total length of character\n",
    "        total_n_tweets+=len(elems)                  # summing the total number of tweets\n",
    "        \n",
    "        n_tweet_dict[pol]+=len(elems)\n",
    "        \n",
    "        #Save all length of tweets\n",
    "        for e in elems:\n",
    "            tweet_lengths_all_dict[pol].append(len(e.split()))\n",
    "            \n",
    "    #Save the total amount of characters for each politician\n",
    "    tweet_length_agg_dict[pol]=sum(tweet_lengths_all_dict[pol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of tweets {}\".format(total_n_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average length of tweets {}\".format(total_string_length/total_n_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a histogram over the sum of the length of all tweets for all politicians using Sturge's rule for the number of bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tweet_length_agg_dict.values()).hist(bins=12)\n",
    "plt.title(\"The total sum of tweet length for politcians\")\n",
    "plt.ylabel('number of politicians')\n",
    "plt.xlabel('Sum of words tweetet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the politicians has a tweetet in total, where others are mentioned, between 0 and 1000 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of the length of the tweets (again using Sturge's rule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2d=list(tweet_lengths_all_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "merged = list(itertools.chain(*list2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(merged).hist(bins=16)\n",
    "plt.title(\"Length of tweets\")\n",
    "plt.ylabel('number of tweets')\n",
    "plt.xlabel('Tweet length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the tweets where politician mentions other politicians is around 20 words long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotting the distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(n_tweet_dict.values())).hist(bins=16)\n",
    "plt.title(\"number of politicians\")\n",
    "plt.ylabel('number of politicians')\n",
    "plt.xlabel('number of tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most of the politicians has between 0 and 25 tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter text statistics (last 200 tweets for every politician)  (LAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_dict_all_v2.data', 'rb') as filehandle:\n",
    "    tweet_dict_onlytext = pickle.load(filehandle) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of twitter profiles who tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(tweet_dict_onlytext.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweets of 642 of the politicians has correctly been collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_n_tweets=0\n",
    "total_string_length=0\n",
    "\n",
    "n_tweet_dict=dict()\n",
    "tweet_lengths_all_dict=dict()\n",
    "tweet_length_agg_dict=dict()\n",
    "\n",
    "for pol in list(tweet_dict_onlytext.keys()):\n",
    "    \n",
    "    tweet_lengths_all_dict[pol]=[]\n",
    "    \n",
    "    n_tweet_dict[pol]=len(tweet_dict_onlytext[pol])\n",
    "    total_n_tweets+=len(tweet_dict_onlytext[pol])\n",
    "    \n",
    "    \n",
    "    for elems in list(tweet_dict_onlytext[pol]):\n",
    "        elems_splittet_to_words=elems.split()\n",
    "        tweet_lengths_all_dict[pol].append(len(elems_splittet_to_words))\n",
    "        \n",
    "        total_string_length+=len(elems_splittet_to_words)        \n",
    "            \n",
    "    #Save the total amount of characters for each politician\n",
    "    tweet_length_agg_dict[pol]=sum(tweet_lengths_all_dict[pol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The total number of tweets {}\".format(total_n_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average length of tweets {}\".format(total_string_length/total_n_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a histogram over the sum of the length of all tweets for all politicians using Sturge's rule for the number of bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tweet_length_agg_dict.values()).hist(bins=12)\n",
    "plt.title(\"The total sum of tweet length for politcians\")\n",
    "plt.ylabel('number of politicians')\n",
    "plt.xlabel('Sum of words tweetet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the politicians has a tweetet in total between 0 and 500 words.\n",
    "Then a \"normal\"-like distribution occur with min around 2000 and max around 7000. Theres a spike around 5000 words tweetet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of the length of the tweets (again using Sturge's rule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2d=list(tweet_lengths_all_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "merged = list(itertools.chain(*list2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(merged).hist(bins=16)\n",
    "plt.title(\"Length of tweets\")\n",
    "plt.ylabel('number of tweets')\n",
    "plt.xlabel('Tweet length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the tweets where is around 20 words long and the distribution-tail flattens out with a maximum at 69 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API has a limit of 200 tweets, but not all politicians neccesarily has tweetet 200 times. \n",
    "Plotting the distribution of number of tweets collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(n_tweet_dict.values())).hist(bins=16)\n",
    "plt.title(\"Number of tweets collected\")\n",
    "plt.ylabel('number of politicians')\n",
    "plt.xlabel('number of tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(list(n_tweet_dict.values()),0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(list(n_tweet_dict.values()),0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array(list(n_tweet_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(a == 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25% of the politicians has 20 or less tweets, but over 50% has 200 tweets.\n",
    "312 politicians have 200 tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meetings text statistics (FREDERIK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network statistics (LAU) & (CHRISTIAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation of the network is based on the previous generated dictionary from [Tweets-(who-tags-who)](#Tweets-(who-tags-who)) section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_dict_listversion_wrealnames_v2.data', 'rb') as filehandle:\n",
    "    tweet_dict = pickle.load(filehandle) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a politician e.g Lars Løkke, mentions another politicians e.g. Morten Østergaard, the two politicians are connected. Each politician represent a node in the graph. The mentioning using a \"@\" makes alink from Lars to Morten, which gives a directed edge from Lars to Morten. Morten does not neccesarily mention Lars in any posts - therefore, to incoporate the complexity of mentioning in Twitter, a directed graph is chosen for the social politician Twitter network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='images/lars_mention_oster.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For generating a graph representing the network of twitter politicians, the NetworkX library is used. The library makes it possible to create a empty directed graphs, and one by one add connections (edges) between the politicians. The labels of the nodes are the politicians' twitternames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the directed graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.DiGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make edge from tweeting politicinas to the ones that they mentions. Before generating the network, it is assumed that if Lars mentions Morten in e.g. 12 tweets, then the edge from Lars to Morten has a weight value of 12.\n",
    "\n",
    "If Lars then only mentions Margrethe Vestager 3 times, the edge from Lars to Vestager is 3. \n",
    "\n",
    "The weight $W_{ij}$ in the graph therefore represent the number of times politician (node) *i* mentions politician (node) *j*.\n",
    "\n",
    "The weight for a edge is then calculated by finding the length of the list of tweets where politician *i* mentions politician *j*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pol in list(tweet_dict.keys()): #All politician, i\n",
    "    for to_pol in list(tweet_dict[pol].keys()): #All that i mentions, j\n",
    "            if G.has_edge(pol, to_pol):\n",
    "                # If there's already a edge, print a error\n",
    "                print(\"error\")\n",
    "            else:\n",
    "                G.add_edge(pol, to_pol, weight=len(tweet_dict[pol][to_pol]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/load file locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.write_gpickle(G, \"second_network_w_weights_listversion_wrealnames_v2.gpickle\")\n",
    "G = nx.read_gpickle(\"second_network_w_weights_listversion_wrealnames_v2.gpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having created the graph, we can now look into the first statistics of the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of nodes: {}\".format(G.number_of_nodes()))\n",
    "print(\"The number of edges: {}\".format(G.number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 650 politicians, 561 of them has mentioned another politician or has been mentioned. Between all of the nodes theres 10445 weighted edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the first glance of the network, networkx is used for plotting the graph. The size of the nodes is in this plot dependent of the degree.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(G.degree(G.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "nx.draw_kamada_kawai(G, node_size=[v * 0.1 for v in d.values()], width=0.1)\n",
    "\n",
    "plt.title(\"Nice visualization of the total network\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Networkx has a tool to give the weighted node degree. From [networkx.DiGraph.degree](https://networkx.org/documentation/stable/reference/classes/generated/networkx.DiGraph.degree.html): \"The weighted node degree is the sum of the edge weights for edges incident to that node.\". Thereby the degree of a node/Politician depends on the number of Tweets the politician has been mentioned in. If the weight has not been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_weight = dict(G.degree(G.nodes(),'weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,20))\n",
    "nx.draw_kamada_kawai(G, node_size=[v * 0.1 for v in d_weight.values(), width=0.1)\n",
    "\n",
    "plt.title(\"Nice visualization of the total network with weight-degree\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of weighted degree has a clear impact on the graph. Looking at Vestager again, the degree value increases by around 85%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_change(current, previous):\n",
    "    if current == previous:\n",
    "        return 100.0\n",
    "    try:\n",
    "        return (abs(current - previous) / previous) * 100.0\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=list(dict(G.degree([\"vestager\"])).values())[0]\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end=list(dict(G.degree([\"vestager\"],'weight')).values())[0]\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_change(end,start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vestager's node in_degree difference between a non-weighted and a weighted is 85.4%. Finding all the difference between all politicians degree and weighted degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_deff_dict={}\n",
    "for node in list(G.nodes()):\n",
    "    start=list(dict(G.degree([node])).values())[0]\n",
    "    end=list(dict(G.degree([node],'weight')).values())[0]\n",
    "    deg_deff_dict.update({node:get_change(end,start)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the highest differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(deg_deff_dict.items(), key=lambda x: x[1], reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"regeringDK degree: {}\".format(G.degree(\"regeringDK\")))\n",
    "print(\"HolmerHans degree: {}\".format(G.degree(\"HolmerHans\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"regeringDK degree: {}\".format(G.degree(\"regeringDK\",'weight')))\n",
    "print(\"HolmerHans degree: {}\".format(G.degree(\"HolmerHans\",'weight')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The danish parliament has its own twitterprofile, which apparently has a lot of mentions. With a degree of 455 it implicates that the profile is very connected. But including the weight it is even more obvoius that alot of tweets from a lot of politicians mentions \"regeringDK\"\n",
    "\n",
    "HolmerHans is Chairman of Social and Health at Kolding City Council. Using only the non-weighted degree, the importance of this twitterprofile could might be underestimated. When accounting for the number of tweets (weight) he suddenly become very actively in the social twitter network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding node attributes\n",
    "\n",
    "We want to add some metadata to this twitter network. Of course, this meta data will be the parties of these politicians, but since we can, also the regions that they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function that ensures G and dataframe are equal sizewise\n",
    "\n",
    "G = nx.read_gpickle(\"second_network_w_weights_listversion_wrealnames.gpickle\")\n",
    "def pre_check():\n",
    "    nodes = G.nodes()\n",
    "    count=0\n",
    "    to_remove = [] \n",
    "    for node in nodes:\n",
    "        if node in data2.values:\n",
    "            count+=1\n",
    "        else:\n",
    "            print(node + \" will be removed\")\n",
    "            to_remove.append(node)\n",
    "    for node in to_remove:\n",
    "        G.remove_node(node)\n",
    "    return len(to_remove)\n",
    "\n",
    "print(\"Removing \" +str(pre_check()) + \" nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we make a dict of attributes of \"party\" and \"region\" are these attributes\n",
    "\n",
    "dict_of_attributes = {}\n",
    "for node in G.nodes():\n",
    "    #print(node)\n",
    "    x = data2.loc[data2['Twittername'] == node]\n",
    "    #print(x)\n",
    "    party =  x.values[0][2]\n",
    "    region = x.values[0][3]\n",
    "    #print(node, party, region)\n",
    "    attribute = {'party': party, 'region': region}\n",
    "    dict_of_attributes[node] = attribute\n",
    "nx.set_node_attributes(G, dict_of_attributes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counting links between parties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parties = list(nx.get_node_attributes(G,'party').values())\n",
    "politicians=list(G.nodes)\n",
    "unique_parties = set(parties)\n",
    "\n",
    "tweet_to_count = {}\n",
    "tweet_out_count = {}\n",
    "\n",
    "\n",
    "for party in unique_parties:\n",
    "    tweet_to_count[party] = 0\n",
    "    tweet_out_count[party] = 0\n",
    "    \n",
    "# party in + out degrees\n",
    "print(\"edges (mentions): \" + str(G.number_of_edges()))\n",
    "for edge in G.edges():\n",
    "    _from = edge[0]\n",
    "    _to = edge[1]\n",
    "    _to_party = parties[politicians.index(_to)]\n",
    "    _out_party = parties[politicians.index(_from)]\n",
    "    \n",
    "    \n",
    "    tweet_to_count[_to_party] += 1\n",
    "    tweet_out_count[_out_party] += 1\n",
    "\n",
    "    \n",
    "tweet_to_count = dict(sorted(tweet_to_count.items(), key=lambda item: item[1], reverse=True))\n",
    "tweet_out_count = dict(sorted(tweet_out_count.items(), key=lambda item: item[1],reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig = plt.subplots(figsize=(15,10))\n",
    "\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(tweet_out_count.keys(), tweet_out_count.values())\n",
    "plt.title(\"Out-degree distribution with UNKNOWN\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(15,10))\n",
    "\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(tweet_to_count.keys(), tweet_to_count.values())\n",
    "plt.title(\"In-degree distribution with UNKNOWN\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without the unknowns** \n",
    "\n",
    "\n",
    "We want to see it without unknowns in the network and also the mean per person, as it's a bit skewed towards the parties that have the most politicians (more politicians = more tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweet_to_count_clean = tweet_to_count.copy()\n",
    "tweet_out_count_clean = tweet_out_count.copy()\n",
    "\n",
    "tweet_to_count_clean.pop('UNKNOWN')\n",
    "tweet_out_count_clean.pop('UNKNOWN')\n",
    "\n",
    "vals = nx.get_node_attributes(G, \"party\").values()\n",
    "sum_party =dict.fromkeys(tweet_out_count.keys(),0)\n",
    "for party_occ in vals:\n",
    "    sum_party[party_occ] += 1\n",
    "sum_party\n",
    "\n",
    "avg_tweet_out = dict.fromkeys(tweet_out_count.keys())\n",
    "avg_tweet_in= dict.fromkeys(tweet_to_count.keys())\n",
    "\n",
    "for key in tweet_out_count:\n",
    "    avg_tweet_out[key] = tweet_out_count[key] / sum_party[key]\n",
    "    avg_tweet_in[key] = tweet_to_count[key] / sum_party[key]\n",
    "    \n",
    "ang_tweet_out = dict(sorted(avg_tweet_out.items(), key=lambda item: item[1],reverse=True))\n",
    "avg_tweet_in = dict(sorted(avg_tweet_in.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "\n",
    "#ax = plt.subplot(111)\n",
    "\n",
    "#x = pd.DataFrame([avg_tweet_in])\n",
    "#chart =sns.barplot(x=\"day\", y=\"total_bill\",data=x)\n",
    "#chart.set_xticklabels(chart.get_xticklabels(), rotation=90)\n",
    "#plt.show()\n",
    "fig = plt.subplots(figsize=(15,10))\n",
    "\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(ang_tweet_out.keys(), ang_tweet_out.values())\n",
    "plt.title(\"Mean out degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.subplots(figsize=(15,10))\n",
    "\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(avg_tweet_in.keys(), avg_tweet_in.values())\n",
    "plt.title(\"Mean in degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how it looks with all the parties added in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gcc = sorted(nx.weakly_connected_components(G), key=len, reverse=True)\n",
    "\n",
    "giant = G.subgraph(Gcc[0])\n",
    "giant = giant.to_undirected()\n",
    "\n",
    "for node in list(giant.nodes()):\n",
    "    if (giant.degree(node) <= 2):\n",
    "        giant.remove_node(node)\n",
    "\n",
    "parties = list(nx.get_node_attributes(giant,'party').values())\n",
    "color_map = []\n",
    "d = dict(giant.degree) #degrees for every node\n",
    "\n",
    "#Making color list\n",
    "for i in range(len(parties)): #for all nodes\n",
    "    if parties[i] == 'Socialdemokratiet':\n",
    "        color_map.append('red')\n",
    "    elif parties[i] == 'Venstre':\n",
    "        color_map.append('blue')\n",
    "    elif parties[i]=='Dansk Folkeparti':\n",
    "        color_map.append('pink')\n",
    "    elif parties[i]=='Socialistisk Folkeparti':\n",
    "        color_map.append('brown')\n",
    "        \n",
    "    elif parties[i] == 'Radikale Venstre':\n",
    "        color_map.append('#1f78b4')\n",
    "        \n",
    "    elif parties[i] == 'Enhedslisten':\n",
    "        color_map.append('#D0004D')\n",
    "        \n",
    "    elif parties[i] == 'Det Konservative Folkeparti':\n",
    "        color_map.append('#00583C')\n",
    "    elif parties[i] == 'Nye Borgerlige':\n",
    "        color_map.append('#235d66')\n",
    "    elif parties[i] == 'Liberal Alliance':\n",
    "        color_map.append('orange')\n",
    "    elif parties[i] == 'Alternativet':\n",
    "        color_map.append('yellow')\n",
    "    elif parties[i] == 'Government':\n",
    "        color_map.append('black')\n",
    "    else:\n",
    "        color_map.append('white')\n",
    "        \n",
    "from fa2 import ForceAtlas2\n",
    "\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=False,  # Dissuade hubs   #Want hubs inside other!\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=2.0,\n",
    "                        strongGravityMode=True,\n",
    "                        gravity=1.0,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(giant, pos=None, iterations=2000)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "nx.draw_networkx_nodes(giant, positions, node_color=color_map, alpha=0.7,node_size=[v * 3 for v in d.values()])\n",
    "nx.draw_networkx_edges(giant, positions, edge_color=\"green\", alpha=0.05)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "The graph is quite compact, with many large blue and red nodes which makes sense, as these are the biggest parties in denmark; namely \"Venstre\" and \"Socialdemokratiet\". Furthermore, there are large black nodes that are central in the network - this also makes sense as we've marked government entities as black. However there are still some problems with the visualization that we want to solve.\n",
    "\n",
    "\n",
    "**Segregating into red and blue blocks**\n",
    "\n",
    "The problem with the network visualization is that we have too many different parties and getting a nice visualization becomes difficult. Bloc politics refers to the act of parties uniting in order to gain a majority vote over the parliament. In danish politics, according to https://da.wikipedia.org/wiki/Blokpolitik we have 3 blocs, red blue and green. We will segregate our parties into this by adding a graph attribute \"bloc\" . Furthermore we will eliminate the unknowns to get a graph that's more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_bloc = ['Socialdemokratiet', 'Enhedslisten', 'Socialistisk Folkeparti', 'Radikale Venstre']\n",
    "blue_bloc = ['Venstre', 'Dansk Folkeparty', 'Det Konservative Folkeparti', 'Liberal Alliance', 'Nye Borgerlige', 'Fremad']\n",
    "green_bloc = ['Alternativet']\n",
    "    \n",
    "    \n",
    "for key, val in dict_of_attributes.items():\n",
    "    #print(key)\n",
    "    if val['party'] in red_bloc:\n",
    "        val['block'] = 'red'\n",
    "    elif val['party'] in blue_bloc:\n",
    "        val['block'] = 'blue'\n",
    "    elif val['party'] in green_bloc:\n",
    "        val['block'] = 'green'\n",
    "    elif val['party'] == 'Government':\n",
    "        val['block'] = 'black'\n",
    "    else: \n",
    "        val['block'] = 'white' # white node means unknown\n",
    "        \n",
    "nx.set_node_attributes(G, dict_of_attributes)\n",
    "\n",
    "Gcc = sorted(nx.weakly_connected_components(G), key=len, reverse=True)\n",
    "\n",
    "giant = G.subgraph(Gcc[0])\n",
    "giant = giant.to_undirected()\n",
    "d = dict(giant.degree) #degrees for every node\n",
    "\n",
    "for node in list(giant.nodes()):\n",
    "    if (giant.degree(node) <= 2):\n",
    "        giant.remove_node(node)\n",
    "\n",
    "block_color_map = list(nx.get_node_attributes(giant, 'block').values())\n",
    "\n",
    "# removing the unknowns\n",
    "for node in list(giant.nodes(data2=True)):\n",
    "    if (node[1]['block'] == 'white'):\n",
    "        giant.remove_node(node[0])\n",
    "block_color_map = list(nx.get_node_attributes(giant, 'block').values())\n",
    "d = dict(giant.degree) #degrees for every node\n",
    "\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=False,  # Dissuade hubs   #Want hubs inside other!\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=2.0,\n",
    "                        strongGravityMode=True,\n",
    "                        gravity=1.0,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(giant, pos=None, iterations=2000)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "nx.draw_networkx_nodes(giant, positions, node_color=block_color_map, alpha=0.7,node_size=[v * 3 for v in d.values()])\n",
    "nx.draw_networkx_edges(giant, positions, edge_color=\"green\", alpha=0.05)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "G = nx.read_gpickle(\"second_network_w_weights_listversion_wrealnames_v2.gpickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network looks a bit neater, but we can observe that the large central government profiles have decreased in degree, signalling that a lot of the unknowns have tweeted to that profile. Interestingly, the few green block nodes are leaning to the right. Now we will look at sentiment of tweets, that link 2 profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "With 561 danish politicians, connected by 10445 edges a directed graph representing a \"Social network of danish politicians on Twitter\" has been created. The graph is directed do to the distinction between politician A mentioning politician B, and the other way around. Using a weighted directed graph emphasizes the importance on the number of tweets on Twitter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools, theory and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis step 1: Network  (LAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to build models that reproduce the properties of real networks. To examine whether the graph reproduces a real network or it is just a random network, the degree distribution of a random network is compared to the Twitter network. \n",
    "\n",
    "By examining the properties of the Twitter network, there can be shed light on how politicians can influence each other. This project assumes, that a politician is interested to know how to influence others most effectively and how to spred a message to the public including the other politicians. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section degree-distributions are going to be plotted. For this purpose the code below can plot a customade histogram. For plotting an appropiate amount of bins [Sturge's formula](https://en.wikipedia.org/wiki/Histogram#Sturges'_formula) is applied. The number of bins is equal to $\\lceil log_{n}\\rceil +1$, where *n* is the total number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_histogram(list_of_values,title_name,x_label,y_label):\n",
    "    \n",
    "    #Sturges rule for number of bins\n",
    "    n_bin=math.ceil(math.log2(len(list_of_values)))+1\n",
    "    \n",
    "    hist,bin_edges=np.histogram(list_of_values,bins=n_bin)\n",
    "\n",
    "    rightboundary=bin_edges[1:]\n",
    "    leftboundary=bin_edges[:-1]\n",
    "    newlist_in=[] # list with leftboundary1,rightboundary1,leftboundary2,rightboundary2\n",
    "    newvalues_in=[] # histvalue1,histvalue1,histvalue2,histvalue2,....\n",
    "    for i in range(len(rightboundary)):\n",
    "        newlist_in.append(leftboundary[i])\n",
    "        newvalues_in.append(hist[i])\n",
    "        newlist_in.append(rightboundary[i])\n",
    "        newvalues_in.append(hist[i]) \n",
    "    points_in=zip(newlist_in,newvalues_in)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(newlist_in,newvalues_in,color='#0504aa')\n",
    "    for pt in points_in:\n",
    "        plt.plot( [pt[0],pt[0]], [0,pt[1]],color=\"#0504aa\" )\n",
    "\n",
    "    plt.title(title_name,fontsize=18)\n",
    "    plt.xlabel(x_label,fontsize=14)\n",
    "    plt.ylabel(y_label,fontsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the in- and out-degree of the graph is assesed. This is done in order to get an idea of how the politicians are connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_in=list(dict(G.in_degree()).values()) # This is all of the nodes' degrees\n",
    "list_out=list(dict(G.out_degree()).values()) # This is all of the nodes' degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_histogram(list_in,\"Binning in-degree distribution\",\"in-degree\",\"Frequency\")\n",
    "custom_histogram(list_out,\"Binning out-degree distribution\",\"out-degree\",\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Network Sciencebook - Chapter 3, sec. 3.2](http://networksciencebook.com/chapter/3):\n",
    "\"A random network consists of N nodes where each node pair is connected with probability p.\"\n",
    "\n",
    "To calculate the probability p of the Politician Twitter Network, the average degree, k, is found. From [Network Sciencebook - Chapter 3, sec. 3.3](http://networksciencebook.com/chapter/3#random-network) eq. 3.3 the following holds:\n",
    "\n",
    "$k = p ( N-1)$\n",
    "\n",
    "so $p = \\frac{k}{N-1}$\n",
    "\n",
    "Since *k* is equal to the average degree, networkx can calculate the value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=np.mean(list(dict(G.degree()).values())) \n",
    "print(k)\n",
    "\n",
    "\n",
    "\n",
    "###Should we use the weighted???\n",
    "\n",
    "#k=np.mean(list(dict(G.degree(G.nodes(),'weight')).values())) \n",
    "#print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of each pair of nodes is connected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=k/(N-1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the random graph from extracted parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_ER=nx.erdos_renyi_graph(N,p,directed=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the degree distribution for the ER graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_in_r=list(dict(G_ER.in_degree()).values()) # This is all of the nodes' degrees\n",
    "list_out_r=list(dict(G_ER.out_degree()).values()) # This is all of the nodes' degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_histogram(list_in_r,\"Binning in-degree distribution for random graph\",\"in-degree\",\"Frequency\")\n",
    "custom_histogram(list_out_r,\"Binning out-degree distribution for random graph\",\"out-degree\",\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "Comparing the distribution of the twitter network and a random graph, it can be concluded, that there a difference in the degree distribution. \n",
    "\n",
    "The twitter network is in-degree distribution is more right skewed than the random network. \n",
    "Even though, the twitter network out-degree has a more normal-like distribution strucutre, there's still a big spike at out-degree between 0 and 5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the giant component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated network should be representing the Danish Politicians Twitter network - therefore, it should not be \"just\" a random graph. \n",
    "\n",
    "The main difference between a random graph and a scale-free network, is the HUBS. According to [Network Science Book - section 5.2](http://networksciencebook.com/chapter/5) new nodes in most real networks prefers tolink to the more conencted node. This process is \"preferential attachment\" Preferential attachment and growth, are the two properties of the emerging of scale-free network. Scale-free networks are networks, which degree distribution is different than the degree distribution of a random network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of two nodes linking, influence the structure of the graph. There are two extreme cases. If p=0, there's zero probability that any node link to another - all nodes are isolated. If p=1, there's 100% probability that any node link to another - the graph is complete, all nodes links to each other. To get insight into how the politcian Twitter network strucures itself and why, the folllowing analysis will examine the link-probability and number of nodes. Once the average node degree exceeds a critical value, the rapid emergence of a large cluster that we call the giant component occurs. \n",
    "\n",
    "There are four different topological distinct regimes according to [Network Science Book - section 3.6](http://networksciencebook.com/chapter/3#evolution-network). One can distinguish between them by looking at their characteristics - more specifically on the relation between p and N. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p >math.log(N)/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regime is a connected regime , because p>ln(N)/N. This indicates that the twitter network will emerge to a giant component, which aborbs all nodes and components. \n",
    "\n",
    "According to [Network Science Book - section 3.7](http://networksciencebook.com/chapter/3#networks-supercritical) most networks are supercritical. But e.g. the real network of Actors is in the connected regime. The twitter network of politicians properties are therefore consistent with the some of the properties of other real world networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critical point seperates the regime where theres is a giant component from the one wheres there's not.\n",
    "For the network to be at a critical point k, the average node degree, must be equal to 1. This also means that pc=1/N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc=1/(N-1) \n",
    "print(\"{:.4f}\".format(pc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linking probability for the twitter network must be 2% for the network to be at the critical point. If the the probability is 2% or lower, all of the politicians are connected in small components. In the aspect of the influence of politics, the twitter network will then become much more \"useless\" for a politician if the linking probability was lower. To get further analyze the giant component of the twitter network, the giant component can be extracted using networkx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm for finding the giant component (and also later using forceatlas to make nice visualizations) needs the graph to be undirected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_undirected=G.to_undirected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the giant component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gcc = sorted(nx.weakly_connected_components(G), key=len, reverse=True)\n",
    "giant_directed=G.subgraph(Gcc[0])\n",
    "giant = G_undirected.subgraph(Gcc[0])\n",
    "d = dict(giant.degree) #Saves the degree for making the nodesize depended of the degreee in plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of nodes: {}\".format(giant_directed.number_of_nodes()))\n",
    "print(\"The number of edges: {}\".format(giant_directed.number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The giant component contained 555 politicians out of 561. This shows how almost all politicians are absorbed in the giant component. Below is a visualization of the component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fa2 import ForceAtlas2\n",
    "\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=True,  # Dissuade hubs   #Want hubs inside other!\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=2.0,\n",
    "                        strongGravityMode=False,\n",
    "                        gravity=1.0,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(giant, pos=None, iterations=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "nx.draw_networkx_nodes(giant, positions, alpha=0.4,node_size=[v * 1 for v in d.values()])\n",
    "nx.draw_networkx_edges(giant, positions, edge_color=\"green\", alpha=0.05)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_in=list(dict(giant_directed.in_degree()).values()) # This is all of the nodes' degrees\n",
    "#list_out=list(dict(giant_directed.out_degree()).values()) # This is all of the nodes' degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_histogram(list_in,\"Binning in-degree distribution for giant component\",\"in-degree\",\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_histogram(list_out,\"Binning out-degree distribution for giant component\",\"out-degree\",\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [Network Science Book - section 4.2](http://networksciencebook.com/chapter/4) the degrees of a random network follows a possion distribution. Therefore, a random network is generated, and the distribution of the degrees can be compared in a normal scale and log-log scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the random graph by initially finding N and p for the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_giant=np.mean(list(dict(giant.degree()).values())) \n",
    "print(k_giant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_giant=giant.number_of_nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of each pair of nodes is connected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_giant=k_giant/(N_giant-1)\n",
    "print(p_giant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the random graph from extracted parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_r = nx.generators.fast_gnp_random_graph(N_giant,p_giant)\n",
    "\n",
    "degree_freq_r = nx.degree_histogram(G_r)\n",
    "degrees_r = range(len(degree_freq_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the random graph degree distribution and the giant component graphs degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig, axs = plt.subplots(2,figsize=(13,13))\n",
    "fig.suptitle('Degree distribution of network',size=25)\n",
    "m=1\n",
    "\n",
    "degree_freq = nx.degree_histogram(giant)\n",
    "degrees = range(len(degree_freq))\n",
    "\n",
    "#####PLOT 1##########\n",
    "axs[0].plot(degrees[m:], degree_freq[m:],'go')\n",
    "axs[0].plot(degrees_r[m:], degree_freq_r[m:],'bo-')\n",
    "green1_patch = mpatches.Patch(color='g', label='Degrees of giant component')\n",
    "blue1_patch = mpatches.Patch(color='b', label='Degrees of random network')\n",
    "axs[0].legend(handles=[blue1_patch,green1_patch])\n",
    "\n",
    "\n",
    "###PLOT 2 ######\n",
    "axs[1].loglog(degrees[m:], degree_freq[m:],'go')\n",
    "axs[1].loglog(degrees_r[m:], degree_freq_r[m:],'bo-')\n",
    "green2_patch = mpatches.Patch(color='g', label='Degrees of giant component')\n",
    "axs[1].legend(handles=[green2_patch])\n",
    "blue2_patch = mpatches.Patch(color='b', label='Degrees of random network')\n",
    "axs[1].legend(handles=[blue2_patch,green2_patch])\n",
    "\n",
    "\n",
    "\n",
    "axs[0].set_ylabel('Number of nodes',fontsize = 14)\n",
    "axs[1].set_xlabel('k',fontsize = 14)\n",
    "axs[1].set_ylabel('Number of nodes',fontsize = 14)\n",
    "\n",
    "plt.subplots_adjust(hspace=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "To yet examine the randomness, the above plots can be inspected. On the log-log plot, the degree distribution of the giant component, the power law follows a approximately straight line. The giant component of the twitter network, is therefore different from the poisson distribution (random network). Since the Twitter network follows the power-law, is is therefore according to [Network Science Book - section 4.2](http://networksciencebook.com/chapter/4) a **scale-free network**. \n",
    "\n",
    "\n",
    "A property of a scale-free network, is the nature of preferential attachment. This affects the degree of the largest node, so new politicians joining danish politics (and Twitter) will prefer to connect to some of the hubs in the network. One or few large hubs connects to most of the notes, which can represent some of the most influental twitter-profiles. Or, at least be a great potential source of bringing a message. The graph is in the connecte regime, and the giant component of the twitter network absorbs all nodes. Therefore, all politicians are somehow connected in the twitter network - the next question is then: \"How are they connected?\" \"Who should a politician address in order to cement a polical agenda?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOUSE OF CARDS - The danish version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Who should a politician address in order to cement a polical agenda?\" This question reminds one of the plot from [House of Cards](https://en.wikipedia.org/wiki/House_of_Cards_(American_TV_series)):\n",
    "\n",
    "\n",
    "\"*Congressman Frank Underwood (Kevin Spacey), a Democrat from South Carolina's 5th congressional district and House Majority Whip, and his equally ambitious wife Claire Underwood (Robin Wright). Frank is passed over for appointment as Secretary of State, so he initiates an elaborate plan to attain power, aided by Claire. The series deals with themes of ruthless pragmatism, manipulation, betrayal, and power.*\"\n",
    "\n",
    "This story is possibly more dramatized than the real life of danish politicians. Nevertheless, this case is called \"House of Cards - the danish version\". If a few politicians on twitter connects to many others, then these could potentially have a great influence on formation of the public opinion - and maybe also the opinion of other politicians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a politician should indirectly address a person through twitter, by talking to a person who knows a person who knows a person..., it is interesting to know what the greatest distance in the network is. This is the same as the diameter of the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.diameter(giant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between any two politicians, the maximum number of people connecting the two, is 6. This number is actually quite interesting, because [Network Science Book - section 3.8](http://networksciencebook.com/chapter/3#small-worlds) describes the \"Small World Phenomenon\": that two individuals anywhere on Earth, you will find a path of at most six acquaintances between them. The twitter network giant has only 555 nodes, so the distance between any two nodes should probably be less than 6. [Network Science Book - section 3.8](http://networksciencebook.com/chapter/3#small-worlds)  states that:\n",
    "\n",
    "< k > nodes should be at distance one\n",
    "\n",
    "$< k >^{2}$ nodes should be at distance two\n",
    "\n",
    "$< k >^{3}$ nodes should be at distance three...\n",
    "\n",
    "where < k > is the average degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"<k>: {}\".format(k_giant))\n",
    "print(\"<k>^2: {}\".format(math.pow(k_giant,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means, that almost all politicians should be in a distance of 2 from eachtoher. The average shortest path can shed more light on this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.average_shortest_path_length(giant)\n",
    "#nx.average_shortest_path_length(giant_directed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the average shortest path between two politicians in the twitter network is approx. 2. \n",
    "\n",
    "The average shortest path of the twitter network confirms, that small world phenomeom holds for the Danish Twitter Network. The following demonstration shows how a random politician are (almost) 2 nodes away from all other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code can generate a plot for finding and visualizing nodes that are *d* nodes away from a node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_d_step_away(node_name,graph,d,color_for_nodes,plotting=\"yes\"):\n",
    "    #Saves the shortest path\n",
    "    values=np.array(list(nx.single_source_shortest_path_length(giant,node_name).values()))\n",
    "    nodess=np.array(list(nx.single_source_shortest_path_length(giant,node_name).keys())) \n",
    "    \n",
    "    #Making a bool-list which detects if the path (or steps) are equal to d:\n",
    "    steps2= [True if element==d else False for element in values]\n",
    "    \n",
    "    #The bool-list can then select the nodes which are d steps away:\n",
    "    \n",
    "    nodes2=nodess[steps2]\n",
    "    \n",
    "    dic=nx.single_source_shortest_path_length(giant,node_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    color_map = []\n",
    "    alpha_list=[]\n",
    "    for node in giant:\n",
    "        if node == node_name:\n",
    "            color_map.append('green')\n",
    "            alpha_list.append(1)\n",
    "        elif node in nodes2:\n",
    "            color_map.append(color_for_nodes)\n",
    "            #alpha_list.append(dic[node]*0.5)\n",
    "            alpha_list.append(1)\n",
    "        else: \n",
    "            color_map.append('grey')  \n",
    "            alpha_list.append(dic[node]*0.2)\n",
    "            \n",
    "    if plotting==\"yes\":\n",
    "        plt.figure(figsize=(10,20))\n",
    "        nx.draw_kamada_kawai(giant,node_size=50, node_color=color_map)\n",
    "        plt.title(\"GCC (Green random node with \"+color_for_nodes +\"coloured nodes \"+str(d)+\" steps away\",fontsize=18)\n",
    "        plt.show()\n",
    "    \n",
    "    return color_map,alpha_list\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a random politician:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "rc=random.choice(list(giant.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create forceatlas positions for the giant connected component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "forceatlas2 = ForceAtlas2(\n",
    "                        # Behavior alternatives\n",
    "                        outboundAttractionDistribution=True,  # Dissuade hubs   #Want hubs inside other!\n",
    "                        linLogMode=False,  # NOT IMPLEMENTED\n",
    "                        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)\n",
    "                        edgeWeightInfluence=1.0,\n",
    "\n",
    "                        # Performance\n",
    "                        jitterTolerance=1.0,  # Tolerance\n",
    "                        barnesHutOptimize=True,\n",
    "                        barnesHutTheta=1.2,\n",
    "                        multiThreaded=False,  # NOT IMPLEMENTED\n",
    "\n",
    "                        # Tuning\n",
    "                        scalingRatio=2.0,\n",
    "                        strongGravityMode=False,\n",
    "                        gravity=1.0,\n",
    "\n",
    "                        # Log\n",
    "                        verbose=True)\n",
    "\n",
    "\n",
    "positions = forceatlas2.forceatlas2_networkx_layout(giant, pos=None, iterations=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the nodes that are 1,2,3,4,5,6 nodes away :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_map,alpha_list=plot_d_step_away(rc,giant,1,\"red\",plotting=\"no\")\n",
    "c_map2,alpha_list2=plot_d_step_away(rc,giant,2,\"blue\",plotting=\"no\")\n",
    "c_map3,alpha_list3=plot_d_step_away(rc,giant,3,\"purple\",plotting=\"no\")\n",
    "c_map4,alpha_list4=plot_d_step_away(rc,giant,4,\"orange\",plotting=\"no\")\n",
    "c_map5,alpha_list5=plot_d_step_away(rc,giant,5,\"magenta\",plotting=\"no\")\n",
    "c_map6,alpha_list6=plot_d_step_away(rc,giant,6,\"cyan\",plotting=\"no\")\n",
    "\n",
    "#Saves all list together:\n",
    "c_map_all=[c_map,c_map2,c_map3,c_map4,c_map5,c_map6]\n",
    "alpha_list_all=[alpha_list,alpha_list2,alpha_list3,alpha_list4,alpha_list5,alpha_list6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make StemClaussons node bigger than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sizes = [50 if node!='StemClausson' else 450 for node in list(giant.nodes())   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a nice GIF for the website, the nodes that are 1-4 steps away from StemClausson is generated (no are more than 4). Then they are uploaded to https://ezgif.com/maker, which genrates the GIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    #Draw the graph\n",
    "    nx.draw_networkx_nodes(giant, positions,node_color=c_map_all[i],node_size=n_sizes)\n",
    "    nx.draw_networkx_edges(giant, positions, edge_color=\"grey\", alpha=0.1)\n",
    "\n",
    "    #Legends\n",
    "    patch1 = Line2D([0], [0], marker='o', color='w', label='1 step away',\n",
    "                        markerfacecolor='r',markersize=20)\n",
    "    patch2 = Line2D([0], [0], marker='o', color='w', label='2 step away',\n",
    "                        markerfacecolor='b',markersize=20)\n",
    "    patch3 = Line2D([0], [0], marker='o', color='w', label='3 step away',\n",
    "                        markerfacecolor='purple',markersize=20)\n",
    "    patch4 = Line2D([0], [0], marker='o', color='w', label='4 step away',\n",
    "                        markerfacecolor='orange',markersize=20)\n",
    "    patch5 = Line2D([0], [0], marker='o', color='w', label='StemClausson',\n",
    "                        markerfacecolor='green',markersize=20)\n",
    "    plt.legend(handles=[patch5,patch1,patch2,patch3,patch4],loc=2,prop={'size': 20},bbox_to_anchor=(0, 0),ncol=5)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('gif_house_of_cards/gif_pic_'+str(i)+'.png')\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot colors for all nodes that are *d* distance away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate video\n",
    "#https://stackoverflow.com/questions/753190/programmatically-generate-video-or-animated-gif-in-python\n",
    "\n",
    "def plot_d_step_away_all(node_name,graph,plotting=\"yes\"):\n",
    "    #Saves the shortest path\n",
    "    \n",
    "    dic=nx.single_source_shortest_path_length(giant,node_name)\n",
    "    \n",
    "    mx=max(dic, key=dic.get)\n",
    "    mx=dic[mx]\n",
    "    \n",
    "    color_list=[\"red\",\"blue\",\"purple\",\"orange\",\"pink\",\"brown\"] #diameter is 6, so only 6 colors\n",
    "      \n",
    "    \n",
    "    color_map = []\n",
    "    alpha_list=[]\n",
    "    for node in list(graph.nodes()):\n",
    "        if node == rc:\n",
    "            color_map.append('green')\n",
    "            alpha_list.append(0.3)\n",
    "        else:\n",
    "            color_map.append(color_list[dic[node]-1]) \n",
    "            alpha_list.append(dic[node]*0.2)\n",
    "            \n",
    "    if plotting==\"yes\":\n",
    "        plt.figure(figsize=(10,20))\n",
    "        nx.draw_kamada_kawai(giant,node_size=50, node_color=color_map)\n",
    "        plt.title(\"GCC (Green random node with coloured nodes d steps away\",fontsize=18)\n",
    "        patch1 = mpatches.Patch(color='r', label='1 step away')\n",
    "        patch2 = mpatches.Patch(color='b', label='2 step away')\n",
    "        patch3 = mpatches.Patch(color='purple', label='3 step away')\n",
    "        patch4 = mpatches.Patch(color='orange', label='4 step away')\n",
    "        patch5 = mpatches.Patch(color='pink', label='5 step away')\n",
    "        patch6 = mpatches.Patch(color='brown', label='6 step away')\n",
    "        \n",
    "        plt.legend(handles=[patch1,patch2,patch3,patch4,patch5,patch6])\n",
    "        \n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    return color_map, alpha_list\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_map_all,alpha_list=plot_d_step_away_all(rc,giant,plotting=\"no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "#nx.draw_networkx_nodes(giant, positions, alpha=alpha_list,node_size=[v * 1 for v in d.values()],node_color=c_map_all)\n",
    "\n",
    "nx.draw_networkx_nodes(giant, positions, alpha=alpha_list,node_color=c_map_all,node_size=50)\n",
    "nx.draw_networkx_edges(giant, positions, edge_color=\"grey\", alpha=0.05)\n",
    "patch1 = mpatches.Patch(color='r', label='1 step away')\n",
    "patch2 = mpatches.Patch(color='b', label='2 step away')\n",
    "patch3 = mpatches.Patch(color='purple', label='3 step away')\n",
    "patch4 = mpatches.Patch(color='orange', label='4 step away')\n",
    "patch5 = mpatches.Patch(color='pink', label='5 step away')\n",
    "patch6 = mpatches.Patch(color='brown', label='6 step away')\n",
    "\n",
    "plt.legend(handles=[patch1,patch2,patch3,patch4,patch5,patch6])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization above shows how the danish politician, StemClausson (green  node), is connected with all the other politicians. Most of the nodes are blue, which means that most of the nodes are two steps away. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say, that a random politician wants to influence one of the most central, according to degree centrality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(giant_directed.degree(), key=lambda x: x[1], reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take \"larsloekke\", then the random politicians shortest path for influence \"larsloekke\". Here, it is assumed, that if \"larsloekke\" mentions politician A, then politican A can influence \"larsloekke\" or at least \"larsloekke\" explicit \"interacts\" with person A. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.shortest_path(giant_directed, source=\"larsloekke\", target=rc))\n",
    "print(\"length: \" +str(len(nx.shortest_path(giant_directed, source=\"larsloekke\", target=rc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the random person, StemClausson, to larsleokke, there are 3 persons between. StemClausson should then tal with mariannefrede20, in order to try to influence \"larsloekke\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another advantage of having the network is, that if StemClausson can NOT directly talk to larsloekke, then he can might talk to some of larsloekke's inner circle. For this purpose, a politician should find the neighboors of larsloekke: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[n for n in giant_directed.neighbors('larsloekke')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StemClausson can then contact one of the above persons, and hope they can influence larsloekke."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if StemClausson want to influence a random politician? Lets draw a random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc2=random.choice(list(giant.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The random politician is: \" +rc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random politician is RasmusNorupC. How are StemClausson and RasmusNorupC connected? Who should StemClausson talk to in order to indirectly make an infulence on RasmusNorupC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=nx.shortest_path(giant_directed, source=rc2, target=rc)\n",
    "print(sp)\n",
    "print(\"length: \" +str(len(sp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.reverse() #Reverse the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From \"+rc +\" to \"+rc2 +\"the following persons should talk with eachother \\n\"+'\\ntalks to...'.join(sp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis step 2: Community detection and wordclouds (LAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify communities in the twitter network, the best_partition algorithm is used, which seeks to maximize the modularity of each community using Louvain heuristics.\n",
    "\n",
    "The highest modularity is found by the highest partition of the dendrogram generated by the Louvain algorithm.\n",
    "\n",
    "The  definition of modularity is defined in http://networksciencebook.com/chapter/9#modularity equation 9.12.\n",
    "\n",
    "$M=\\sum_{c=1}^{n_c} [\\frac{L_c}{L}-(\\frac{k_c}{2L})^2]$\n",
    "\n",
    "where $L_c$ is the total number of links within the community $C_c$ and $k_c$ is the total degree of the nodes in this community.\n",
    "\n",
    "$n_c$ is the number of communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Loading the partitions\n",
    "with open('communities.data', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    partition_GCC=pickle.laod(partition_GCC, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partition_GCC = community.best_partition(giant) #Giant connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(partition_GCC.values())),' communities is found in total.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The giant component has been divided into 17 communities. Here's a distribution of the community-sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "a=np.array(list(partition_GCC.values()))\n",
    "b = Counter(a)\n",
    "\n",
    "#Extracting the number of nodes in each commnuity\n",
    "community_sizes=[i[1] for i in b.most_common(17)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_histogram(community_sizes,\"Distribution of community sizes\",\"community size\",\"frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most of the communities contains between 2 and 14 politicians. \n",
    "Two of the communities has between 55 and 83 politicians. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SAVING THE COMMUNITIES TO A FILE\n",
    "#with open('communities.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "#    pickle.dump(partition_GCC, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating all politicians tweets for each politician"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For making wordcloud that could give insight into which topics the politicians in the communities tweets about, a processing of the real text is made. A TF-IDF transformation of the word is made, then wordclouds shows the words that characterize the communities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing the tf-idf transformation, all the tweets for each politician is concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('tweet_dict_all_v2.data', 'rb') as filehandle:\n",
    " #   tweet_dict_onlytext = pickle.load(filehandle)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_concat=dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pol in list(tweet_dict_onlytext.keys()):\n",
    "    dict_concat[pol]=' '.join(tweet_dict_onlytext[pol])\n",
    "    dict_concat[pol] = re.sub(r\"http\\S+\", \"\", dict_concat[pol]) #remove links\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('tweet_concat_v2.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "#    pickle.dump(dict_concat, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tweet_concat_v2.data', 'rb') as filehandle:\n",
    "    dict_concat = pickle.load(filehandle)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the TF list, both english and danish stopwords are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_danish = stopwords.words('danish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS=STOPWORDS.union(stopwords_danish)\n",
    "STOPWORDS=STOPWORDS.union([\"rt\"])# RT is some twitter syntax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_communities=list(set(partition_GCC.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_text =  dict()\n",
    "TF_dc = dict()\n",
    "\n",
    "\n",
    "for i in range(len(largest_communities)):\n",
    "\n",
    "   \n",
    "    common_characters = [k for k,v in list(partition_GCC.items()) if v == largest_communities[i]]\n",
    "\n",
    "    # reset collection of words for each community\n",
    "    dc_words=[]\n",
    "\n",
    "    # Tokenize the text in all files in community i and concatenate into one text\n",
    "    for pol in list(dict_concat.keys()):\n",
    "            if pol in common_characters:\n",
    "              \n",
    "                dc_words += nltk.word_tokenize(dict_concat[pol])\n",
    "    \n",
    "    # enter all text from community i in a dict\n",
    "    dc_tokens = [word.lower() for word in dc_words if (word.isalnum() and word.lower() not in STOPWORDS)]\n",
    "    #words_lemmatized=[lemmatizer.lemmatize(\"\",t)[0] for t in dc_tokens]\n",
    "    \n",
    "    community_text[largest_communities[i]] = dc_tokens\n",
    "\n",
    "for i in community_text.keys():\n",
    "    TF_dc[i] = FreqDist(community_text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words and text\n",
    "common_communities=largest_communities\n",
    "for i in range(len(common_communities)):\n",
    "    print('Community ', common_communities[i],' has ', len(community_text[common_communities[i]]), ' words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDF**\n",
    "\n",
    "To calculate the TF-IDF we multiply TF and IDF with each other. TF is the raw count of any word in each community and IDF is the *Inverse Document Frequency Smooth*: $log\\frac{N}{n_t+1}+1$,\n",
    "where $N$ is the total number of communities and $n_t$ is the total number of communities which contains the term $t$.\n",
    "\n",
    "\n",
    "The motivation behind using this formula is that it gives a high score for a word that is frequent in few communities and a low score for words that are frequent in many communities. This way, the result indicates words that are especially significant to the individual communities and hence relevant in a wordcloud.\n",
    "\n",
    "\n",
    "We use the raw count (TF weighting scheme), because we expect the wikitext in the communities to be rather similar, mainly differing in the mentioning and description of specific characters and catch phrases. Furthermore, stopwords are excluded. If these were included it might have been preferable to use another measure for TF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverse document frequency\n",
    "def IDF_for_word(word, this_com_name, TF_other_community, N):\n",
    " \n",
    "    nt=0\n",
    " \n",
    "    for com in list(TF_other_community.keys()):\n",
    "        if word in TF_other_community[com] and com != this_com_name:\n",
    "            nt = nt+1\n",
    " \n",
    "    w_IDF=np.log(N/(nt+1))+1\n",
    "    return w_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF = dict()\n",
    "\n",
    "for community in TF_dc.keys():\n",
    "    IDF.update({community: {}})\n",
    "    for word in TF_dc[community].keys():\n",
    "        IDF[community].update({word: IDF_for_word(word, community, TF_dc, 7)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF = dict()\n",
    "\n",
    "for community in TF_dc.keys():\n",
    "    TF_IDF.update({community: {}})\n",
    "    for word in TF_dc[community].keys():\n",
    "        TF_IDF[community].update({word: TF_dc[community][word]*IDF[community][word]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_mask=np.array(Image.open('twitter_sil.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS=STOPWORDS.union(['dkpol','se','få','komme'])# RT is some twitter syntax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flere verber: http://fjern-uv.dk/250.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWs=set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for community in TF_IDF.keys():\n",
    "    community_WordCloudText = ' '.join([word for word in list(TF_IDF[community].keys()) for i in range(math.ceil(TF_IDF[community][word]))])\n",
    "\n",
    "    plt.figure(figsize=(10,20))\n",
    "    community_wordcloud = WordCloud(collocations=False, mask=twitter_mask, background_color='white', stopwords=SWs).generate(community_WordCloudText)\n",
    "    plt.imshow(community_wordcloud, interpolation='bilinear')\n",
    "    plt.savefig('com_wordclouds/community'+str(community)+'.png')\n",
    "    plt.title(label=('Community '+str(community)))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the 8 largest communities to have an appropiate amount of politicians in each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_communities=[i for i in b.most_common(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "largest_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_com_6=giant.subgraph([k for k in list(partition_GCC.keys()) if partition_GCC[k]==6 ])\n",
    "G_com_3=giant.subgraph([k for k in list(partition_GCC.keys()) if partition_GCC[k]== 3])\n",
    "G_com_0=giant.subgraph([k for k in list(partition_GCC.keys()) if partition_GCC[k]==0 ])\n",
    "G_com_4=giant.subgraph([k for k in list(partition_GCC.keys()) if partition_GCC[k]==4 ])\n",
    "G_com_12=giant.subgraph([k for k in list(partition_GCC.keys()) if partition_GCC[k]==12 ])\n",
    "G_com_8=giant.subgraph([k for k in list(partition_GCC.keys()) if partition_GCC[k]==8])\n",
    "G_com_9=giant.subgraph([k for k in list(partition_GCC.keys()) if partition_GCC[k]==9 ])\n",
    "G_com_1=giant.subgraph([k for k in list(partition_GCC.keys()) if partition_GCC[k]==1 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Community 6 is named: \"+' & '.join([x[0] for x in sorted(G_com_6.degree, key=lambda x: x[1], reverse=True)[:3]]))\n",
    "print(\"Community 3 is named: \"+' & '.join([x[0] for x in sorted(G_com_3.degree, key=lambda x: x[1], reverse=True)[:3]]))\n",
    "print(\"Community 0 is named: \"+' & '.join([x[0] for x in sorted(G_com_0.degree, key=lambda x: x[1], reverse=True)[:3]]))\n",
    "print(\"Community 4 is named: \"+' & '.join([x[0] for x in sorted(G_com_4.degree, key=lambda x: x[1], reverse=True)[:3]]))\n",
    "print(\"Community 12 is named: \"+' & '.join([x[0] for x in sorted(G_com_12.degree, key=lambda x: x[1], reverse=True)[:3]]))\n",
    "print(\"Community 8 is named: \"+' & '.join([x[0] for x in sorted(G_com_8.degree, key=lambda x: x[1], reverse=True)[:3]]))\n",
    "print(\"Community 9 is named: \"+' & '.join([x[0] for x in sorted(G_com_9.degree, key=lambda x: x[1], reverse=True)[:3]]))\n",
    "print(\"Community 1 is named: \"+' & '.join([x[0] for x in sorted(G_com_1.degree, key=lambda x: x[1], reverse=True)[:3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_val_to_name={\n",
    "    6:'regeringDK & sophieloehde & JakobEllemann',\n",
    "    3: 'FrankJensenKBH & nikogrunfeld & Jonasbjorn',\n",
    "    0: 'oestergaard & sofiecn & KHegaard',\n",
    "    4: 'larsloekke & Kristian_Jensen & EvaKjerHansen',\n",
    "    12: 'Heunicke & stephanie_lose & SophieHAndersen',\n",
    "    8:'jacobmark_sf & PiaOlsen & Carl__Valentin',\n",
    "    9:'PSkipperEL & PerClausen3 & RosaLundEl',\n",
    "    1:'SchaldemoseMEP & MarianneVind & JanEJoergensen'\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "The 6 community with regeringDK, sophielehde and JakobElleman, talks about \"randerspol\", \"regeringen\", \"horsenspol\"\n",
    "\n",
    "\n",
    "\n",
    "The 3 community with FrankJensenKBH & marvesterager & nikogrunfeld, talks about \"koebenhavner\", \"friegronne\", \"fannybroholm\", \"kbhpol\"\n",
    "\n",
    "\n",
    "The 0 community with oestergaard & sofiecn & KHegaard, talks about \"radikale\", \"helsingørpol\", \"opdagdanmark\", \"gin\"\n",
    "\n",
    "\n",
    "The 4 community with larsloekke & Kristian_Jensen & engelschmidt, talks about \"DRtrekanten\", \"kolding\", \"igniteesportdk\", \"sønderborgpol\"\n",
    "\n",
    "The 12 community with Heunicke & stephanie_lose & Astridkrag, talks about \"regionerne\", \"regionsjaelland\", \"sundpol\", \"dksocial\",\"nsrsygehuse\"\n",
    "\n",
    "The 8 community with jacobmark_sf & PiaOlsen & Carl__Valentin, talks about \"sfkolding\", \"sfpolitik\", \"parkinson\", \"merekisser\",\"sf\"\n",
    "\n",
    "The 9 community with PSkipperEL & PerClausen3 & RosaLundEl, talks about \"aalpol\", \"enhedslisten\", \"pelledragsted\", \"xrp1procent\"\n",
    "\n",
    "The 1 community with MarianneVind & karmel80 & JanEJoergensen, talks about \"eudk\", \"ep\", \"eurosportdk\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting the communities into clusters in a nx Graph, inspiration has been taken from this [StackOverflow question](https://stackoverflow.com/questions/43541376/how-to-draw-communities-with-networkx). First creating a graphs, containing the nodes of the largest comunities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_with=[k for k in list(partition_GCC.keys()) if partition_GCC[k] in [i[0] for i in largest_communities] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giant2=giant.subgraph([k for k in list(partition_GCC.keys()) if partition_GCC[k] in [i[0] for i in largest_communities] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_GCC_shorterlist = { your_key: partition_GCC[your_key] for your_key in have_with }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(list(partition_GCC_shorterlist.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=[\"red\",\"darkorange\",\"olivedrab\",\"yellow\",\"cyan\",\"blue\",\"darkviolet\",\"lime\"]\n",
    "val_to_col={0:0,1:1,3:2,4:3,6:4,8:5,9:6,12:7}\n",
    "\n",
    "color_list_custom=[col[val_to_col[n]] for n in list(partition_GCC_shorterlist.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def community_layout(g, partition):\n",
    "    \"\"\"\n",
    "    Compute the layout for a modular graph.\n",
    "\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    g -- networkx.Graph or networkx.DiGraph instance\n",
    "        graph to plot\n",
    "\n",
    "    partition -- dict mapping int node -> int community\n",
    "        graph partitions\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pos -- dict mapping int node -> (float x, float y)\n",
    "        node positions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pos_communities = _position_communities(g, partition, scale=3.)\n",
    "\n",
    "    pos_nodes = _position_nodes(g, partition, scale=1.)\n",
    "\n",
    "    # combine positions\n",
    "    pos = dict()\n",
    "    for node in g.nodes():\n",
    "        pos[node] = pos_communities[node] + pos_nodes[node]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _position_communities(g, partition, **kwargs):\n",
    "\n",
    "    # create a weighted graph, in which each node corresponds to a community,\n",
    "    # and each edge weight to the number of edges between communities\n",
    "    between_community_edges = _find_between_community_edges(g, partition)\n",
    "\n",
    "    communities = set(partition.values())\n",
    "    hypergraph = nx.DiGraph()\n",
    "    hypergraph.add_nodes_from(communities)\n",
    "    for (ci, cj), edges in between_community_edges.items():\n",
    "        hypergraph.add_edge(ci, cj, weight=len(edges))\n",
    "\n",
    "    # find layout for communities\n",
    "    pos_communities = nx.spring_layout(hypergraph, **kwargs)\n",
    "\n",
    "    # set node positions to position of community\n",
    "    pos = dict()\n",
    "    for node, community in partition.items():\n",
    "        pos[node] = pos_communities[community]\n",
    "\n",
    "    return pos\n",
    "\n",
    "def _find_between_community_edges(g, partition):\n",
    "\n",
    "    edges = dict()\n",
    "\n",
    "    for (ni, nj) in g.edges():\n",
    "        ci = partition[ni]\n",
    "        cj = partition[nj]\n",
    "\n",
    "        if ci != cj:\n",
    "            try:\n",
    "                edges[(ci, cj)] += [(ni, nj)]\n",
    "            except KeyError:\n",
    "                edges[(ci, cj)] = [(ni, nj)]\n",
    "\n",
    "    return edges\n",
    "\n",
    "def _position_nodes(g, partition, **kwargs):\n",
    "    \"\"\"\n",
    "    Positions nodes within communities.\n",
    "    \"\"\"\n",
    "\n",
    "    communities = dict()\n",
    "    for node, community in partition.items():\n",
    "        try:\n",
    "            communities[community] += [node]\n",
    "        except KeyError:\n",
    "            communities[community] = [node]\n",
    "\n",
    "    pos = dict()\n",
    "    for ci, nodes in communities.items():\n",
    "        subgraph = g.subgraph(nodes)\n",
    "        pos_subgraph = nx.spring_layout(subgraph, **kwargs)\n",
    "        pos.update(pos_subgraph)\n",
    "\n",
    "    return pos\n",
    "\n",
    "def test():\n",
    "    # to install networkx 2.0 compatible version of python-louvain use:\n",
    "    # pip install -U git+https://github.com/taynaud/python-louvain.git@networkx2\n",
    "    from community import community_louvain\n",
    "\n",
    "    g = giant2\n",
    "    #partition = community_louvain.best_partition(g) #If no partition has been made\n",
    "    pos = community_layout(g, partition_GCC_shorterlist)\n",
    "    d = dict(g.degree(g.nodes()))\n",
    "\n",
    "    plt.figure(figsize=(30,10))\n",
    "    #nx.draw(g, pos,alpha=0.5, node_color=list(partition.values()),node_size=[v * 10 for v in d.values()]); plt.show()\n",
    "    \n",
    "    nx.draw_networkx_edges(g, pos, alpha=0.1)\n",
    "    #nx.draw_networkx_nodes(g, pos, alpha=0.9,node_size=50,node_color=list(partition_GCC_shorterlist.values()))\n",
    "    nx.draw_networkx_nodes(g, pos, alpha=0.9,node_size=50,node_color=color_list_custom)\n",
    "    #nx.draw_networkx_nodes(g, pos, alpha=0.35,node_color=list(partition.values()),node_size=[v * 3 for v in d.values()])\n",
    "    \n",
    "    #Plot labels\n",
    "    list_set=list(set(list(partition_GCC_shorterlist.values())))\n",
    "    patches=[]\n",
    "    for i in list_set:\n",
    "        patch = mpatches.Patch(color=col[val_to_col[i]], label='Community: '+str(community_val_to_name[i]))\n",
    "        patches.append(patch)\n",
    "        \n",
    "    plt.legend(handles=patches,loc=2,prop={'size': 20},bbox_to_anchor=(0, 0),ncol=2)\n",
    "   \n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis step 3: Parties and wordclouds (CHRISTIAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis step 4:  Parliament+twitter topics (FREDERIK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis step 5: Sentiment (CHRISTIAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis step 6: Tying together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mette Frederiksen er feks ikke med...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidspunkter for tweets vs. tider for meeetings.\n",
    "\n",
    "Kan politikere skifte holdninger..\n",
    "\n",
    "\n",
    "Flere tweets?\n",
    "\n",
    "Forskel på hvor mange tweets folk har"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"You should write (just briefly) which group member was the main responsible for which elements of the assignment. (I want you guys to understand every part of the assignment, but usually there is someone who took lead role on certain portions of the work. That’s what you should explain).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Christian:**<p>\n",
    "All of the following section has Christian has the main responsible for the following sections. For these sections, Christian has done all coding + all vizualisations, text regarding methods/theory and conclusions on all of the analysises.\n",
    "    \n",
    "Explainer notebook<p>  \n",
    "    \n",
    "* Describe datasets: Describe Politicians (parties and regions)\n",
    "* Data cleaning: Politicians (parties and regions) \n",
    "* Tools, theory and analysis: Analysis step 3: Parties and wordclouds\n",
    "* Tools, theory and analysis: Analysis step 5: Sentiment \n",
    "\n",
    "Website <p>\n",
    "    \n",
    "* dddd\n",
    "* ddddd\n",
    "\n",
    "**Frederik:**<p>\n",
    "All of the following section has Frederik has the main responsible for the following sections. For these sections, Frederik has done all coding + all vizualisations, text regarding methods/theory and conclusions on all of the analysises.\n",
    "    \n",
    "Explainer notebook<p> \n",
    "\n",
    "* Describe datasets: Describe Political meetings \n",
    "* Data cleaning: Political meetings\n",
    "* Preliminary statistics, Text statistics (twitter and meetings): Meetings text statistics \n",
    "* Tools, theory and analysis: \"Analysis step 4: Parliament+twitter topics\" \n",
    "\n",
    "\n",
    "Website <p>\n",
    "    \n",
    "* dddd\n",
    "* ddddd\n",
    "\n",
    "**Lau:**<p>\n",
    "All of the following section has Lau has the main responsible for the following sections. For these sections, Lau has done all coding + all vizualisations, text regarding methods/theory and conclusions on all of the analysises.\n",
    "\n",
    "Explainer notebook<p> \n",
    "\n",
    "* Describe dataset -> Describe Twitter\n",
    "* Datacleaning: Twitterprofiles\n",
    "* Datacleaning: Tweets (who tags who) [Tweets-(who-tags-who)](#Tweets-(who-tags-who))\n",
    "* Datacleaning: Tweets (only text)\n",
    "* Preliminary statistics, Text statistics (twitter and meetings): Twitter text statistics (who mentions who)\n",
    "* Preliminary statistics, Text statistics (twitter and meetings): Twitter text statistics (last 200 tweets for every politician)\n",
    "* Preliminary statistics: Network statistics\n",
    "* Tools, theory and analysis: \"Analysis step 1: Network\"\n",
    "* Tools, theory and analysis: \"Analysis step 2: Community detection and wordclouds\"\n",
    "\n",
    "   \n",
    "Website <p>\n",
    "    \n",
    "* Data, Twitterdata\n",
    "* Network analysis: Preliminary statistics\n",
    "* Network analysis: Community detection\n",
    "* Network analysis: House of card - the danish version\n",
    "\n",
    "\n",
    "   \n",
    "   \n",
    "   \n",
    "   \n",
    "\n",
    "MANGLER\n",
    "Analysis step 6: Tying together\n",
    "Discussion\n",
    "\n",
    "Laus website plots!\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If degree weight should in the light of these results in fact also depend on the in_degree. One could assume that \n",
    "\n",
    "#Maybe the weights should be normalized by the number of edges conneting to the node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_weight = dict(G.degree(G.nodes(),'weight'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_in = dict(G.in_degree(G.nodes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_norm=dict()\n",
    "#for node in list(d_weight.keys()):\n",
    "#    if d_in[node]==0:\n",
    "#        d_norm.update({node:d_weight[node]})\n",
    "#    else:\n",
    "#        d_norm.update({node:d_weight[node]/d_in[node]})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start=list(dict(G.degree([\"vestager\"])).values())[0]\n",
    "#print(start)\n",
    "#end=list(dict(G.degree([\"vestager\"],'weight')).values())[0]\n",
    "#print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_norm[\"vestager\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10,20))\n",
    "#nx.draw_kamada_kawai(G, node_size=[v * 0.1 for v in d_norm.values()], width=0.1)\n",
    "\n",
    "#plt.title(\"Nice visualization of the total network with normalized weight-degree\",fontsize=18)\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
