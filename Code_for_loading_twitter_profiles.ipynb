{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Download all twitter politicians"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following webpage has all the danish politician twitterprofiles  https://filip.journet.sdu.dk/twitter/politikere/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re #https://developers.google.com/edu/python/regular-expressions\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "URL = 'https://filip.journet.sdu.dk/twitter/politikere/'\n",
    "page = requests.get(URL)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.content, 'html.parser')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Selecting the twitternames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "See that the twitter names is in a h3 style:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "twitter_names = soup.find_all('h3')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#twitter_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "twitter_name_list=[]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for names in twitter_names:\n",
    "   # print(names, end='\\n'*2)\n",
    "    match=re.search(\"<h3>#\",str(names))\n",
    "    if match:\n",
    "        result_name=re.findall(\".com/\\w*\",str(names))[0].replace(\".com/\",\"\")\n",
    "        twitter_name_list.append(result_name.rstrip())\n",
    "        print(result_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The real names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The real names is in a < small > class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "real_name_list=[]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "real_names = soup.find_all('small')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for r_names in real_names:\n",
    "    result=re.search(r'<small>.+',str(r_names)).group(0).replace(\"<small>\",\"\")\n",
    "    real_name_list.append(result.rstrip())\n",
    "    print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(real_name_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Making dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d={'Names':real_name_list,\"Twittername\":twitter_name_list}\n",
    "data=pd.DataFrame(data=d)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.to_csv(\"danish_politkere_twitter.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finding policial parties from https://www.danskepolitikere.dk/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load csv\n",
    "data = pd.read_csv(\"danish_politkere_twitter.csv\")\n",
    "data = data.drop(columns=['Unnamed: 0'])\n",
    "soup = BeautifulSoup(requests.get('https://www.danskepolitikere.dk/oversigt/danske-politikere').content, 'html.parser')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sections = list(soup.find_all('section'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add \"UNKNOWN\" as default party\n",
    "data['Party'] = 'UNKNOWN'\n",
    "data['Region'] = 'UNKNOWN'\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parties = []\n",
    "# cross reference for our politicians and check if he's in the party\n",
    "for row, coloumns in data.iterrows(): \n",
    "    name = coloumns[0] \n",
    "    for section in sections:\n",
    "        party = section.h3.a.text # gets the party \n",
    "        if re.search(name.lower(), str(section).lower()):\n",
    "            data.at[row, 'Party'] = party\n",
    "            print(\"Found \" + name + \" in \" + party)\n",
    "            \n",
    "        \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# view results\n",
    "data.groupby('Party').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get snapshot of who's missing\n",
    "data.loc[data['Party'] == \"UNKNOWN\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Too many still unkown so we must look elsewhere, in denmark we have many\n",
    "regions with politicans that aren't necsarily members of the parlament\n",
    "![Regioner i dk](https://rn.dk/-/media/Rn_dk/Genveje/Fakta-om-Nordjylland/Regioner-i-Danmark/Danmark_Regioner_2017.ashx?la=da)\n",
    "So we will scrape these websites as well\n",
    "\n",
    "\n",
    "### first north jutland"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nordjylland_url = \"https://rn.dk/da/Politik/De-regionale-politikere\"\n",
    "\n",
    "soup = BeautifulSoup(requests.get(nordjylland_url).content, 'html.parser')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# examine html\n",
    "#print(soup.prettify())\n",
    "match_rnj = []\n",
    "pre_re = r\"\\b[A-Z].*\"\n",
    "div_lsit = soup.findAll(\"ul\", {'class': 'span3'})\n",
    "for paragraph in div_lsit:\n",
    "    text= paragraph.text\n",
    "    text_stripped = text.rstrip().lstrip()\n",
    "    #print(text_stripped)\n",
    "    matches_1 = re.findall(pre_re, text_stripped)\n",
    "    print(matches_1)\n",
    "    match_rnj.append(matches_1)\n",
    "\n",
    "# 0 and even indices are names, 1 and odd are the matching parti strings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# note that relevant matches come at last 2 indices\n",
    "good_match = match_rnj[len(match_rnj)-2] + match_rnj[len(match_rnj)-1]\n",
    "\n",
    "print(good_match)\n",
    "# remove strings that contain \"formand\" as it's not relevant information\n",
    "good_match = [x for x in good_match if \"formand\" not in x.lower() if \"\\r\" not in x.lower()]\n",
    "print(good_match)\n",
    "print(len(good_match))\n",
    "\n",
    "for i in range(len(good_match)-1):\n",
    "    name = good_match[i]\n",
    "    party_string = good_match[i+1]\n",
    "    if name in data.values:\n",
    "        print(name+ \"found in \"+ party_string.replace(\"Parti: \", \"\"))\n",
    "        data.loc[data['Names'] == name, 'Party'] = party_string.replace(\"Parti: \", \"\")\n",
    "        data.loc[data['Names'] == name, 'Region'] = 'nordjylland'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.groupby('Party').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Region Midtjylland"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = 'https://www.rm.dk/politik/regionsradsmedlemmer/'\n",
    "\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "content = soup.find(\"div\", {\"class\": \"text-content\"})\n",
    "\n",
    "unwanted = content.findAll(\"p\")\n",
    "for u in unwanted:\n",
    "    u.clear()\n",
    "\n",
    "# removing some introductioniary text\n",
    "unwanted = content.find(\"h2\")\n",
    "unwanted.clear()\n",
    "unwanted.find_next().clear()\n",
    "clean_content = content.text.rstrip()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# removing unwanted brackets ()\n",
    "pattern = r\"\\(.*?\\)\"\n",
    "clean_content = re.sub(pattern, '', clean_content)\n",
    "\n",
    "\n",
    "#print(clean_content) # left with this string"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_parties = content.findAll(\"h3\")\n",
    "for party in all_parties:\n",
    "    #print(party.text)\n",
    "    party_text = re.sub(pattern,'', party.text)\n",
    "    people = party.find_next('ul')\n",
    "    people_text = re.sub(pattern, \"\", people.text)\n",
    "    for name in people_text.split('\\n'):\n",
    "        if name in data.values:\n",
    "            print(name + \"  in   \" + party_text)\n",
    "            data.loc[data['Names'] == name, 'Party'] = party_text.rstrip()\n",
    "            data.loc[data['Names'] == name, 'Region'] = 'midtjylland'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.groupby('Party').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.Party.unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Region syddammark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = 'https://www.regionsyddanmark.dk/wm436558'\n",
    "req = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(req.content, 'html.parser')\n",
    "content = soup.find(\"div\", {\"class\": \"brodtekst\"})\n",
    "\n",
    "unwanted = content.findAll(\"p\")\n",
    "for u in unwanted:\n",
    "    u.clear()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_parties = content.findAll(\"h3\")\n",
    "i = 0\n",
    "for party in all_parties:\n",
    "        party_text = re.sub(pattern,'', party.text)\n",
    "        people = party.find_next('ul')\n",
    "        people_text = re.sub(pattern, \"\", people.text)\n",
    "        for people in people_text.split('\\n'):\n",
    "            if people.strip() in data.values:\n",
    "                print(people.strip() + \"  in   \" + party_text.strip())\n",
    "                data.loc[data['Names'] == people.strip(), 'Party'] = party_text.strip()\n",
    "                data.loc[data['Names'] == name, 'Region'] = 'syddanmark'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.groupby('Party').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Region Sjælland"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = 'https://www.regionsjaelland.dk/politik/regionsraadet/regionsraadets-medlemmer/Sider/default.aspx'\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "content = soup.findAll('div',{'class': 'employeList'})\n",
    "content = content[1:]\n",
    "for name_content in content:\n",
    "    name = name_content.a['title']\n",
    "    spans = name_content.findAll('span')\n",
    "    party = spans[1]\n",
    "\n",
    "    party_text = party.text.split(',')[1]\n",
    "    party_text = re.sub(pattern, '', party_text)\n",
    "    if name.strip() in data.values:\n",
    "        print(name.strip() + \"  in   \" + party_text.strip())\n",
    "        data.loc[data['Names'] == name.strip(), 'Party'] = party_text.strip()\n",
    "        data.loc[data['Names'] == name, 'Region'] = 'sjælland'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.groupby('Party').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Region Hovedstaden"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = 'https://www.regionh.dk/politik/politiske-udvalg-og-fora/regionsraadet/Sider/Medlemmer_af_regionsraadet_2018-2021.aspx'\n",
    "\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.content, 'html.parser')\n",
    "content = soup.findAll('h2', {'class': 'rh-Element-H2A'})\n",
    "\n",
    "for party in content:\n",
    "    print(party.text)\n",
    "    party_text = re.sub(pattern, '', party.text)\n",
    "    print(party_text.strip())\n",
    "    print(\"\")\n",
    "    names = party.find_all_next('h3', {'rh-Element-H3A'})\n",
    "    # bit weird website structure compared to others so differnet method is used\n",
    "    # each party returns the names of politicians + those listed after it\n",
    "    # we can update each name setting it to the party continously for each element\n",
    "    for name in names:\n",
    "        name_text = name.text\n",
    "        name_text = name_text.strip()\n",
    "        #print(name_text)\n",
    "        if name_text in data.values:\n",
    "                print(name_text + \" in \" + party_text.strip())\n",
    "                data.loc[data['Names'] == name_text, 'Party'] = party_text.strip()\n",
    "                data.loc[data['Names'] == name, 'Region'] = 'hovedstaden'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.groupby('Party').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get snapshot of who's missing\n",
    "data.loc[data['Party'] == \"UNKNOWN\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finally from ft.dk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### nordjyllands storkreds\n",
    "def getFromFt(url: str, region: str):\n",
    "    req=requests.get(url)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    memberInfos = soup.findAll('tr')\n",
    "    # skip table header\n",
    "    memberInfos = memberInfos[1:]\n",
    "    print(\"Found \" + str(len(memberInfos)))\n",
    "    for member in memberInfos:\n",
    "        # format: '\\n\\n -> first_name -> last_name -> party\n",
    "        dataList = [x.text for x in member.findAll('td')]\n",
    "        try:\n",
    "            _, firstName, lastName, party, _, _ = dataList\n",
    "            name = firstName + \" \" + lastName\n",
    "            if name in data.values:\n",
    "                print(\"In twitter csv: \" + name + \" \" + party)\n",
    "                print(\"Old party: \" + data.loc[data['Names']==name]['Party'])\n",
    "                data.loc[data['Names'] == name, 'Party'] = party\n",
    "                data.loc[data['Names'] == name, 'Region'] = region\n",
    "\n",
    "        except:\n",
    "            print(\"failed for\")\n",
    "            print(dataList)\n",
    "\n",
    "\n",
    "\n",
    "url = 'https://www.ft.dk/searchResults.aspx?sortedDescending=false&consti=%7bD68D9A59-8A4B-426D-AD9C-9B32FE3CE071%7d&page=1&sortedBy=&pageSize=200'\n",
    "getFromFt(url, 'nordjylland')\n",
    "data.groupby('Party').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Vestjyllandsstokkreds + øst\n",
    "# trying same code\n",
    "\n",
    "url = 'https://www.ft.dk/searchResults.aspx?sortedDescending=false&consti={97A50F7E-B9D0-4D91-924F-AB20235DB4FD}'\n",
    "getFromFt(url, 'midtjylland')\n",
    "data.groupby('Party').count()\n",
    "\n",
    "# østjylland\n",
    "url = 'https://www.ft.dk/searchResults.aspx?sortedDescending=false&consti={E3910753-B21F-4E97-AD2B-67A9A3C15B62}'\n",
    "getFromFt(url, 'midtjylland')\n",
    "data.groupby('Party').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# syddanmark\n",
    "url = 'https://www.ft.dk/searchResults.aspx?sortedDescending=false&consti={7D7567F4-3425-42A1-A035-2FB9880A5F81}'\n",
    "getFromFt(url, 'syddanmark')\n",
    "data.groupby('Party').count()\n",
    "\n",
    "\n",
    "url ='https://www.ft.dk/searchResults.aspx?sortedDescending=false&consti={B3A26626-55E1-4BE2-8503-E4027904E25A}'\n",
    "getFromFt(url, 'syddanmark')\n",
    "data.groupby('Party').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "url = 'https://www.ft.dk/searchResults.aspx?sortedDescending=false&consti=%7b79B9CCF4-4793-4835-8E68-6770AA11A85D%7d&page=1&sortedBy=&pageSize=50'\n",
    "getFromFt(url, 'sjælland')\n",
    "data.groupby('Party').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Region hovedstadan\n",
    "url1 = 'https://www.ft.dk/searchResults.aspx?sortedDescending=false&consti={4C1C732F-4F3C-408E-A009-C96E6934B29C}'\n",
    "url2 = 'https://www.ft.dk/searchResults.aspx?sortedDescending=false&consti={3F642D1B-1820-48F1-A288-AA1FE9079640}'\n",
    "url3 = 'https://www.ft.dk/searchResults.aspx?sortedDescending=false&consti={6A4F6A85-C5C1-4656-BD9A-7831094A5A81}'\n",
    "url4 = 'https://www.ft.dk/searchResults.aspx?sortedDescending=false&consti={E76B8CF6-E8A7-4B29-9813-350C49E36238}'\n",
    "\n",
    "getFromFt(url1, 'hovedstadan')\n",
    "getFromFt(url2, 'hovedstadan')\n",
    "getFromFt(url3, 'hovedstadan')\n",
    "getFromFt(url4, 'hovedstadan')\n",
    "\n",
    "data.groupby('Party').count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyzing why there are still so many unknowns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# unknowns = data.loc[data['Party'] == \"UNKNOWN\"]\n",
    "#\n",
    "# for row in unknowns['Names']:\n",
    "#     print(row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks like there are several reasons, some names have emojis. Some have misplaced whitespaces, sometimes more\n",
    "sometimes 0. There are some names in the list that are entities and not people - such as AMPJ or various ministeries.\n",
    "There can also be trouble finding politicians because of the capitalization (easy fix) and laslty,\n",
    "some middle names are abbreviated and some aren't; making it harder to know and find the specific politician.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# write to csv\n",
    "data.to_csv('danish_politkere_twitter2.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}